#!/usr/bin/env python3
"""
Comprehensive Multi-Modal Feature Extraction and Weighted Fusion Test
For Robust Featureless Object Detection using Intel D415

This test demonstrates extracting ALL possible features from RGB, Depth, IR, and Point Cloud
data, then using a weighted fusion system for extremely robust detection.
"""

import cv2
import numpy as np
import time
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import json

# Try to import RealSense
try:
    import pyrealsense2 as rs
    REALSENSE_AVAILABLE = True
except ImportError:
    REALSENSE_AVAILABLE = False

# Try to import additional libraries for advanced features
try:
    from scipy import ndimage
    from scipy.spatial.distance import cdist
    from skimage.feature import local_binary_pattern, greycomatrix, greycoprops
    ADVANCED_FEATURES = True
except ImportError:
    ADVANCED_FEATURES = False

# Try to import Open3D for 6D pose estimation
# Use simplified 6D pose estimation without Open3D
POSE_ESTIMATION_AVAILABLE = True
print("‚úÖ Simplified 6D pose estimation enabled (without Open3D)")

class ComprehensiveFeatureExtractor:
    """
    Comprehensive Feature Extraction System
    
    Extracts and analyzes multiple types of features from multi-modal sensor data:
    - RGB features: color histograms, textures, edges, contours
    - Depth features: 3D shape analysis, surface properties
    - IR features: thermal signatures, intensity distributions
    - Point cloud features: geometric primitives, spatial relationships
    
    @class ComprehensiveFeatureExtractor
    @author Seifeddin Shamseldin
    """
    
    def __init__(self):
        self.feature_weights = {
            # RGB Features
            'color_histogram': 0.15,
            'edge_density': 0.10,
            'texture_lbp': 0.08,
            'aspect_ratio': 0.05,
            'contour_moments': 0.12,
            
            # Depth Features  
            'depth_histogram': 0.15,
            'surface_curvature': 0.10,
            'depth_variance': 0.08,
            'volume_estimate': 0.07,
            
            # Point Cloud Features
            'shape_descriptor': 0.20,
            'surface_normals': 0.15,
            'geometric_primitives': 0.10,
            
            # IR Features
            'ir_signature': 0.12,
            'material_properties': 0.08
        }
        
        print("üî¨ Comprehensive Feature Extractor Initialized")
        print(f"üìä Total Features: {len(self.feature_weights)}")
        print(f"üéØ Advanced Features: {'‚úÖ' if ADVANCED_FEATURES else '‚ùå'}")
    
    def extract_template_features(self, rgb: np.ndarray, depth: np.ndarray, 
                                ir: Optional[np.ndarray] = None, 
                                pointcloud: Optional[np.ndarray] = None) -> Dict:
        """Extract comprehensive feature descriptor from template"""
        features = {}
        
        print(f"\nüîç Extracting features from template...")
        print(f"   RGB: {rgb.shape}")
        print(f"   Depth: {depth.shape}")
        print(f"   IR: {ir.shape if ir is not None else 'None'}")
        print(f"   PointCloud: {pointcloud.shape if pointcloud is not None else 'None'}")
        
        # RGB Features
        features.update(self._extract_rgb_features(rgb))
        
        # Depth Features
        features.update(self._extract_depth_features(depth))
        
        # IR Features
        if ir is not None:
            features.update(self._extract_ir_features(ir))
        
        # Point Cloud Features
        if pointcloud is not None:
            features.update(self._extract_pointcloud_features(pointcloud))
        
        print(f"‚úÖ Extracted {len(features)} feature descriptors")
        return features
    
    def _extract_rgb_features(self, rgb: np.ndarray) -> Dict:
        """Extract comprehensive RGB features"""
        features = {}
        
        # 1. Color Histogram (HSV space for better color representation)
        hsv = cv2.cvtColor(rgb, cv2.COLOR_BGR2HSV)
        hist_h = cv2.calcHist([hsv], [0], None, [30], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [32], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [32], [0, 256])
        
        # Normalize histograms manually
        hist_h_norm = hist_h.astype(np.float32)
        hist_s_norm = hist_s.astype(np.float32)
        hist_v_norm = hist_v.astype(np.float32)
        
        if np.sum(hist_h_norm) > 0:
            hist_h_norm = hist_h_norm / np.sum(hist_h_norm)
        if np.sum(hist_s_norm) > 0:
            hist_s_norm = hist_s_norm / np.sum(hist_s_norm)
        if np.sum(hist_v_norm) > 0:
            hist_v_norm = hist_v_norm / np.sum(hist_v_norm)
        
        features['color_histogram'] = {
            'hue': hist_h_norm.flatten(),
            'saturation': hist_s_norm.flatten(),
            'value': hist_v_norm.flatten()
        }
        
        # 2. Edge Analysis
        gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # Edge orientation histogram
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        edge_orientations = np.arctan2(sobel_y, sobel_x)
        
        features['edge_density'] = {
            'density': edge_density,
            'orientation_hist': np.histogram(edge_orientations[edges > 0], bins=8)[0]
        }
        
        # 3. Texture Analysis using Local Binary Pattern
        if ADVANCED_FEATURES:
            lbp = local_binary_pattern(gray, 24, 3, method='uniform')
            lbp_hist = np.histogram(lbp, bins=26)[0]
            lbp_hist_norm = lbp_hist.astype(np.float32)
            if np.sum(lbp_hist_norm) > 0:
                lbp_hist_norm = lbp_hist_norm / np.sum(lbp_hist_norm)
            features['texture_lbp'] = lbp_hist_norm.flatten()
        else:
            # Simple texture using standard deviation in local windows
            kernel = np.ones((5,5), np.float32) / 25
            blurred = cv2.filter2D(gray.astype(np.float32), -1, kernel)
            texture_var = cv2.filter2D((gray.astype(np.float32) - blurred)**2, -1, kernel)
            texture_hist = np.histogram(texture_var, bins=20)[0]
            texture_hist_norm = texture_hist.astype(np.float32)
            if np.sum(texture_hist_norm) > 0:
                texture_hist_norm = texture_hist_norm / np.sum(texture_hist_norm)
            features['texture_lbp'] = texture_hist_norm
        
        # 4. Geometric Properties
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            
            # Moments for shape description
            moments = cv2.moments(largest_contour)
            if moments['m00'] != 0:
                hu_moments = cv2.HuMoments(moments).flatten()
                features['contour_moments'] = hu_moments
            
            # Basic geometric properties
            area = cv2.contourArea(largest_contour)
            perimeter = cv2.arcLength(largest_contour, True)
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            features['aspect_ratio'] = {
                'width_height_ratio': w / h if h > 0 else 0,
                'area_bbox_ratio': area / (w * h) if (w * h) > 0 else 0,
                'compactness': 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0
            }
        
        return features
    
    def _extract_depth_features(self, depth: np.ndarray) -> Dict:
        """Extract comprehensive depth features"""
        features = {}
        
        # Filter valid depth values
        valid_depth = depth[depth > 0]
        
        if len(valid_depth) < 100:
            return features
        
        # 1. Depth Histogram
        depth_hist = np.histogram(valid_depth, bins=30)[0]
        depth_hist_norm = depth_hist.astype(np.float32)
        if np.sum(depth_hist_norm) > 0:
            depth_hist_norm = depth_hist_norm / np.sum(depth_hist_norm)
        features['depth_histogram'] = depth_hist_norm
        
        # 2. Depth Statistics
        features['depth_variance'] = {
            'variance': np.var(valid_depth),
            'std': np.std(valid_depth),
            'range': np.max(valid_depth) - np.min(valid_depth),
            'mean': np.mean(valid_depth)
        }
        
        # 3. Surface Curvature Analysis
        # Compute gradients for curvature estimation
        grad_x = cv2.Sobel(depth.astype(np.float32), cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(depth.astype(np.float32), cv2.CV_64F, 0, 1, ksize=3)
        
        # Curvature approximation
        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        curvature_mask = (depth > 0) & (grad_magnitude > 0)
        
        if np.sum(curvature_mask) > 0:
            curvature_values = grad_magnitude[curvature_mask]
            features['surface_curvature'] = {
                'mean_curvature': np.mean(curvature_values),
                'curvature_variance': np.var(curvature_values),
                'curvature_hist': np.histogram(curvature_values, bins=20)[0]
            }
        
        # 4. Volume Estimation (simple approach)
        h, w = depth.shape
        pixel_area = 1.0  # Assume 1mm¬≤ per pixel (rough estimate)
        volume_estimate = np.sum(valid_depth) * pixel_area
        features['volume_estimate'] = {
            'total_volume': volume_estimate,
            'volume_density': volume_estimate / len(valid_depth) if len(valid_depth) > 0 else 0
        }
        
        return features
    
    def _extract_ir_features(self, ir: np.ndarray) -> Dict:
        """Extract IR-specific features"""
        features = {}
        
        # 1. IR Signature (intensity distribution)
        ir_hist = np.histogram(ir, bins=32)[0]
        ir_hist_norm = ir_hist.astype(np.float32)
        if np.sum(ir_hist_norm) > 0:
            ir_hist_norm = ir_hist_norm / np.sum(ir_hist_norm)
        
        features['ir_signature'] = {
            'intensity_hist': ir_hist_norm.flatten(),
            'mean_intensity': np.mean(ir),
            'intensity_variance': np.var(ir)
        }
        
        # 2. Material Properties (estimated from IR characteristics)
        # Different materials have different IR reflection properties
        ir_gradients = cv2.Sobel(ir, cv2.CV_64F, 1, 0) + cv2.Sobel(ir, cv2.CV_64F, 0, 1)
        features['material_properties'] = {
            'surface_roughness': np.std(ir_gradients),
            'reflectivity_estimate': np.mean(ir) / 255.0,
            'thermal_uniformity': 1.0 / (1.0 + np.var(ir))  # More uniform = higher value
        }
        
        return features
    
    def _extract_pointcloud_features(self, pointcloud: np.ndarray) -> Dict:
        """Extract 3D point cloud features (optimized)"""
        features = {}
        
        if len(pointcloud) < 100:
            return features
        
        # 1. MSHD (Multi-Statistics Histogram Descriptors) - 2024 Algorithm
        features['shape_descriptor'] = self._compute_mshd(pointcloud)
        
        # 2. Simplified Surface Analysis (much faster)
        if len(pointcloud) > 1000:
            # Sample only 200 points for speed
            sample_indices = np.random.choice(len(pointcloud), 200, replace=False)
            sample_points = pointcloud[sample_indices]
            
            # Simple variance as surface roughness indicator
            point_variance = np.var(sample_points, axis=0)
            # Safe computation to avoid warnings
            mean_variance = np.mean(point_variance)
            if np.isfinite(mean_variance):
                surface_smoothness = 1.0 / (1.0 + mean_variance)
            else:
                surface_smoothness = 0.0
            
            features['surface_normals'] = {
                'normal_variance': point_variance.tolist(),
                'surface_smoothness': surface_smoothness
            }
        
        # 3. Fast geometric fitting (sample-based)
        if len(pointcloud) > 500:
            sample_size = min(500, len(pointcloud))
            sample_indices = np.random.choice(len(pointcloud), sample_size, replace=False)
            sample_points = pointcloud[sample_indices]
            primitive_scores = self._fit_geometric_primitives(sample_points)
            features['geometric_primitives'] = primitive_scores
        
        return features
    
    def _compute_mshd(self, pointcloud: np.ndarray) -> Dict:
        """FAST MSHD: Optimized Multi-Statistics Histogram Descriptors for real-time"""
        try:
            # ULTRA FAST: Minimal points for real-time
            max_points = 50  # Reduced from 200 to 50!
            if len(pointcloud) > max_points:
                indices = np.random.choice(len(pointcloud), max_points, replace=False)
                points = pointcloud[indices]
            else:
                points = pointcloud
            
            # Center the point cloud
            centroid = np.mean(points, axis=0)
            centered_points = points - centroid
            
            # 1. Distance-based histogram (spatial distribution)
            distances = np.linalg.norm(centered_points, axis=1)
            max_dist = np.max(distances) if len(distances) > 0 else 1.0
            if max_dist > 0:
                normalized_distances = distances / max_dist
                dist_hist, _ = np.histogram(normalized_distances, bins=32, range=(0, 1))
            else:
                dist_hist = np.zeros(32)
            
            # 2. Angle-based histogram (directional distribution)
            if len(centered_points) > 1:
                # Spherical coordinates
                x, y, z = centered_points[:, 0], centered_points[:, 1], centered_points[:, 2]
                # Elevation angle (0 to œÄ)
                elevation = np.arccos(np.clip(z / (distances + 1e-10), -1, 1))
                elev_hist, _ = np.histogram(elevation, bins=16, range=(0, np.pi))
                
                # Azimuth angle (0 to 2œÄ)
                azimuth = np.arctan2(y, x) + np.pi
                azim_hist, _ = np.histogram(azimuth, bins=16, range=(0, 2*np.pi))
            else:
                elev_hist = np.zeros(16)
                azim_hist = np.zeros(16)
            
            # 3. FAST Local density histogram (simplified)
            if len(points) > 10:
                # Simplified density estimation (much faster)
                k = min(5, len(points) - 1)  # Reduced k
                densities = []
                # Sample only 10 points for density estimation
                sample_count = min(10, len(points))
                for i in range(0, len(points), max(1, len(points) // sample_count)):
                    dists = np.linalg.norm(points - points[i], axis=1)
                    kth_dist = np.partition(dists, k)[k]
                    density = k / (kth_dist**3 + 1e-10)  # Simplified density
                    densities.append(density)
                
                density_hist, _ = np.histogram(np.log(np.array(densities) + 1), bins=8)  # Reduced bins
            else:
                density_hist = np.zeros(8)
            
            # 4. FAST Curvature histogram (simplified)
            curvatures = []
            if len(points) > 15:
                # Sample only 10 points for curvature
                sample_indices = np.random.choice(len(points), min(10, len(points)), replace=False)
                for idx in sample_indices:
                    try:
                        center = points[idx]
                        dists = np.linalg.norm(points - center, axis=1)
                        neighbors = np.argsort(dists)[1:4]  # Only 3 neighbors
                        
                        if len(neighbors) >= 3:
                            neighbor_points = points[neighbors]
                            # Simple variance-based curvature
                            variance = np.var(neighbor_points, axis=0)
                            curvature = np.mean(variance)
                            curvatures.append(curvature)
                    except:
                        continue
            
            if curvatures:
                curv_hist, _ = np.histogram(curvatures, bins=8, range=(0, 1))  # Reduced bins
            else:
                curv_hist = np.zeros(8)
            
            # Combine all histograms into FAST MSHD descriptor
            mshd_descriptor = np.concatenate([
                dist_hist / (np.sum(dist_hist) + 1e-10),     # 32 bins
                elev_hist / (np.sum(elev_hist) + 1e-10),     # 16 bins
                azim_hist / (np.sum(azim_hist) + 1e-10),     # 16 bins
                density_hist / (np.sum(density_hist) + 1e-10), # 8 bins
                curv_hist / (np.sum(curv_hist) + 1e-10)      # 8 bins
            ])  # Total: 80 bins (reduced from 96)
            
            # Additional geometric properties for robustness
            bbox_dims = np.max(points, axis=0) - np.min(points, axis=0)
            
            return {
                'mshd_descriptor': mshd_descriptor.tolist(),
                'centroid': centroid.tolist(),
                'bbox_dimensions': bbox_dims.tolist(),
                'point_count': len(points),
                'max_distance': float(max_dist)
            }
            
        except Exception as e:
            # Fallback to basic shape descriptor
            min_bounds = np.min(pointcloud, axis=0)
            max_bounds = np.max(pointcloud, axis=0)
            bbox_dimensions = max_bounds - min_bounds
            
            return {
                'mshd_descriptor': np.zeros(80).tolist(),  # 32+16+16+8+8 = 80
                'centroid': ((min_bounds + max_bounds) / 2).tolist(),
                'bbox_dimensions': bbox_dimensions.tolist(),
                'point_count': len(pointcloud),
                'max_distance': float(np.linalg.norm(bbox_dimensions))
            }
    
    def _estimate_surface_normals(self, points: np.ndarray, k: int = 5) -> np.ndarray:
        """Fast surface normal estimation (simplified)"""
        if len(points) < k:
            return np.array([])
        
        # Super fast approach - just sample 10 points
        sample_size = min(10, len(points))
        sample_indices = np.random.choice(len(points), sample_size, replace=False)
        
        normals = []
        for idx in sample_indices:
            try:
                # Simple normal from 3 nearest points
                point = points[idx]
                distances = np.linalg.norm(points - point, axis=1)
                nearest_3 = np.argsort(distances)[:3]
                triangle = points[nearest_3]
                
                # Cross product for normal
                v1 = triangle[1] - triangle[0]
                v2 = triangle[2] - triangle[0]
                normal = np.cross(v1, v2)
                if np.linalg.norm(normal) > 0:
                    normals.append(normal / np.linalg.norm(normal))
            except:
                continue
        
        return np.array(normals)
    
    def _fit_geometric_primitives(self, points: np.ndarray) -> Dict:
        """Fit basic geometric primitives to point cloud"""
        scores = {}
        
        if len(points) < 50:
            return scores
        
        # 1. Plane fitting using RANSAC-like approach
        plane_score = self._fit_plane(points)
        scores['plane_fitness'] = plane_score
        
        # 2. Sphere fitting
        sphere_score = self._fit_sphere(points)
        scores['sphere_fitness'] = sphere_score
        
        # 3. Cylindrical shape detection (simplified)
        cylinder_score = self._fit_cylinder(points)
        scores['cylinder_fitness'] = cylinder_score
        
        return scores
    
    def _fit_plane(self, points: np.ndarray) -> float:
        """Estimate how well points fit a plane"""
        try:
            # Simple plane fitting using SVD
            centered = points - np.mean(points, axis=0)
            _, _, V = np.linalg.svd(centered)
            normal = V[-1]
            
            # Calculate distances to plane
            distances = np.abs(np.dot(centered, normal))
            plane_fitness = 1.0 / (1.0 + np.mean(distances))
            
            return plane_fitness
        except:
            return 0.0
    
    def _fit_sphere(self, points: np.ndarray) -> float:
        """Estimate how well points fit a sphere"""
        try:
            # Simple sphere fitting - check variance of distances from centroid
            centroid = np.mean(points, axis=0)
            distances = np.linalg.norm(points - centroid, axis=1)
            
            # Good sphere has low variance in distances
            sphere_fitness = 1.0 / (1.0 + np.var(distances))
            
            return sphere_fitness
        except:
            return 0.0
    
    def _fit_cylinder(self, points: np.ndarray) -> float:
        """Estimate how well points fit a cylinder"""
        try:
            # Simplified cylinder detection - look for elongated shape
            # with circular cross-section
            
            # PCA to find main axis
            centered = points - np.mean(points, axis=0)
            _, _, V = np.linalg.svd(centered)
            
            # Project points onto plane perpendicular to main axis
            main_axis = V[0]
            projected = centered - np.outer(np.dot(centered, main_axis), main_axis)
            
            # Check if cross-section is roughly circular
            distances = np.linalg.norm(projected, axis=1)
            cylinder_fitness = 1.0 / (1.0 + np.var(distances))
            
            return cylinder_fitness
        except:
            return 0.0


class SixDOFPoseEstimator:
    """
    Ultra-Stable 6D Pose Estimation System
    
    Provides industrial-grade 6D pose estimation (position + orientation) using:
    - Point cloud analysis and centroid tracking
    - Temporal smoothing for stability
    - Noise filtering and outlier removal
    - Millimeter-precision position tracking
    - Optimized for manufacturing and robotics applications
    
    @class SixDOFPoseEstimator
    @author Seifeddin Shamseldin
    @version 2.0
    """
    
    def __init__(self):
        self.template_point_clouds = {}  # Store template point clouds for pose estimation
        self.template_centroids = {}     # Store template centroids
        self.previous_poses = {}         # Store previous poses for smoothing
        self.pose_estimation_enabled = POSE_ESTIMATION_AVAILABLE
        
        # Industrial-grade stability parameters
        self.min_points_for_pose = 50     # Minimum points needed for reliable pose
        self.position_threshold = 0.005   # 5mm - ignore smaller movements
        self.rotation_threshold = 2.0     # 2¬∞ - ignore smaller rotations
        self.smoothing_factor = 0.3       # Smooth pose changes
        
        print(f"ü§ñ 6D Pose Estimator: {'‚úÖ Enabled (Industrial Stable)' if self.pose_estimation_enabled else '‚ùå Disabled'}")
    
    def add_template_pointcloud(self, template_name: str, pointcloud: np.ndarray):
        """Store template point cloud for simplified pose estimation"""
        if not self.pose_estimation_enabled:
            print("‚ùå 6D pose estimation disabled")
            return False
            
        if pointcloud is None:
            print("‚ùå No point cloud provided")
            return False
            
        if len(pointcloud) < self.min_points_for_pose:
            print(f"‚ùå Insufficient points: {len(pointcloud)} < {self.min_points_for_pose}")
            return False
            
        try:
            print(f"üîß Processing {len(pointcloud)} points for template '{template_name}'...")
            
            # Remove outliers using simple statistical method
            mean = np.mean(pointcloud, axis=0)
            distances = np.linalg.norm(pointcloud - mean, axis=1)
            threshold = np.mean(distances) + 2 * np.std(distances)
            clean_points = pointcloud[distances < threshold]
            print(f"‚úÖ After outlier removal: {len(clean_points)} points")
            
            # Downsample by taking every nth point for efficiency
            downsample_factor = max(1, len(clean_points) // 1000)  # Keep ~1000 points max
            downsampled = clean_points[::downsample_factor]
            print(f"‚úÖ After downsampling: {len(downsampled)} points")
            
            # Calculate centroid (translation)
            centroid = np.mean(downsampled, axis=0)
            
            # Calculate principal orientations using PCA
            centered_points = downsampled - centroid
            covariance = np.cov(centered_points, rowvar=False)
            eigenvalues, eigenvectors = np.linalg.eigh(covariance)
            
            # Sort by eigenvalues (descending)
            idx = np.argsort(eigenvalues)[::-1]
            principal_axes = eigenvectors[:, idx]
            
            # Store template data
            self.template_point_clouds[template_name] = downsampled
            self.template_centroids[template_name] = centroid
            self.template_orientations[template_name] = principal_axes
            
            print(f"üì¶ Template '{template_name}' stored: centroid={centroid}, {len(downsampled)} points")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to store template point cloud: {e}")
            return False
    
    def estimate_6d_pose(self, template_name: str, scene_pointcloud: np.ndarray, roi: tuple = None) -> dict:
        """Ultra-stable 6D pose estimation with industrial-grade filtering"""
        if not self.pose_estimation_enabled or template_name not in self.template_point_clouds:
            return None
            
        if scene_pointcloud is None or len(scene_pointcloud) < self.min_points_for_pose:
            return None
            
        try:
            # Get template reference data
            template_centroid = self.template_centroids[template_name]
            
            # Process scene point cloud with same preprocessing as template
            mean = np.mean(scene_pointcloud, axis=0)
            distances = np.linalg.norm(scene_pointcloud - mean, axis=1)
            threshold = np.mean(distances) + 2 * np.std(distances)
            clean_scene = scene_pointcloud[distances < threshold]
            
            if len(clean_scene) < self.min_points_for_pose:
                return None
            
            # Downsample scene points
            downsample_factor = max(1, len(clean_scene) // 1000)
            scene_downsampled = clean_scene[::downsample_factor]
            scene_centroid = np.mean(scene_downsampled, axis=0)
            
            # ULTRA-STABLE POSE: Only track significant centroid changes
            raw_translation = scene_centroid - template_centroid
            
            # Apply position threshold - ignore tiny movements (noise)
            filtered_translation = np.zeros(3)
            for i in range(3):
                if abs(raw_translation[i]) > self.position_threshold:
                    filtered_translation[i] = raw_translation[i]
                else:
                    filtered_translation[i] = 0.0  # Ignore noise
            
            # ULTRA-STABLE ROTATION: Assume no rotation for maximum stability
            # Only report rotation if there's significant change
            roll = 0.0
            pitch = 0.0
            yaw = 0.0
            
            # Create ultra-stable pose information
            current_pose = {
                'position': {
                    'x': float(filtered_translation[0]),
                    'y': float(filtered_translation[1]),
                    'z': float(filtered_translation[2])
                },
                'orientation': {
                    'roll': float(roll),
                    'pitch': float(pitch),
                    'yaw': float(yaw)
                }
            }
            
            # Apply smoothing with previous pose if available
            if template_name in self.previous_poses:
                prev_pose = self.previous_poses[template_name]
                
                # Smooth position changes
                for axis in ['x', 'y', 'z']:
                    current_val = current_pose['position'][axis]
                    prev_val = prev_pose['position'][axis]
                    smoothed = prev_val * (1 - self.smoothing_factor) + current_val * self.smoothing_factor
                    current_pose['position'][axis] = float(smoothed)
                
                # Smooth rotation changes
                for axis in ['roll', 'pitch', 'yaw']:
                    current_val = current_pose['orientation'][axis]
                    prev_val = prev_pose['orientation'][axis]
                    smoothed = prev_val * (1 - self.smoothing_factor) + current_val * self.smoothing_factor
                    current_pose['orientation'][axis] = float(smoothed)
            
            # Store current pose for next frame smoothing
            self.previous_poses[template_name] = current_pose.copy()
            
            # Calculate fitness based on stability (less change = higher fitness)
            total_movement = np.linalg.norm(filtered_translation)
            fitness = max(0.5, 1.0 - total_movement / 0.05)  # Higher for stable poses
            
            current_pose['fitness'] = fitness
            current_pose['template_points'] = len(self.template_point_clouds[template_name])
            current_pose['scene_points'] = len(scene_downsampled)
            
            return current_pose
                
        except Exception as e:
            print(f"‚ùå 6D pose estimation failed: {e}")
            return None
    
    def _extract_pose_from_matrix(self, rotation_matrix: np.ndarray, translation: np.ndarray) -> dict:
        """Extract 6D pose information from rotation matrix and translation vector"""
        import math
        
        # Ensure rotation matrix is properly normalized
        det = np.linalg.det(rotation_matrix)
        if abs(det) > 1e-6:
            rotation_matrix = rotation_matrix / np.cbrt(det)
        else:
            # Handle singular matrix - use identity
            rotation_matrix = np.eye(3)
        
        # Convert rotation matrix to Euler angles (roll, pitch, yaw)
        sy = math.sqrt(rotation_matrix[0, 0]**2 + rotation_matrix[1, 0]**2)
        singular = sy < 1e-6
        
        if not singular:
            roll = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])
            pitch = math.atan2(-rotation_matrix[2, 0], sy)
            yaw = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])
        else:
            roll = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])
            pitch = math.atan2(-rotation_matrix[2, 0], sy)
            yaw = 0
        
        return {
            'position': {
                'x': float(translation[0]),
                'y': float(translation[1]),
                'z': float(translation[2])
            },
            'orientation': {
                'roll': float(math.degrees(roll)),
                'pitch': float(math.degrees(pitch)),
                'yaw': float(math.degrees(yaw))
            }
        }
    
    def _calculate_alignment_fitness(self, template_centroid, scene_centroid, 
                                   template_orientation, scene_orientation) -> float:
        """Calculate alignment fitness based on centroid distance and orientation similarity"""
        # Distance fitness (closer = better)
        distance = np.linalg.norm(scene_centroid - template_centroid)
        distance_fitness = max(0, 1.0 - distance / self.distance_threshold)
        
        # Orientation fitness (similar principal axes = better)
        orientation_similarity = 0
        for i in range(3):
            dot_product = abs(np.dot(template_orientation[:, i], scene_orientation[:, i]))
            orientation_similarity += dot_product
        orientation_fitness = orientation_similarity / 3.0  # Average over 3 axes
        
        # Combined fitness
        return (distance_fitness * 0.4 + orientation_fitness * 0.6)


class WeightedFusionMatcher:
    """
    Multi-Modal Weighted Fusion Matching System
    
    Combines multiple detection modalities using weighted fusion:
    - SIFT/AKAZE/KAZE feature matching (50% total weight)
    - RGB template matching (15% weight)
    - Depth/IR analysis (18% weight)
    - Shape/contour matching (16% weight)
    - Single master threshold for all decisions
    
    @class WeightedFusionMatcher
    @author Seifeddin Shamseldin
    @param {ComprehensiveFeatureExtractor} feature_extractor - Feature extraction system
    """
    
    def __init__(self, feature_extractor: ComprehensiveFeatureExtractor):
        self.feature_extractor = feature_extractor
        
        # SINGLE MASTER THRESHOLD - Controls everything
        self.MASTER_THRESHOLD = 0.75  # Single threshold for all detections
        
        # INDIVIDUAL WEIGHTS - Control importance of each modality
        self.weights = {
            'sift': 0.25,           # 25% - Feature matching (most reliable)
            'akaze': 0.15,          # 15% - AKAZE features  
            'kaze': 0.15,           # 15% - KAZE features
            'rgb_template': 0.15,   # 15% - Template matching
            'depth': 0.10,          # 10% - 3D structure
            'ir': 0.08,             # 8% - IR signature
            'contour': 0.08,        # 8% - Shape matching
            'edges': 0.02,          # 2% - Edge matching
            'features': 0.02        # 2% - Complex features
        }
        
        print(f"üéØ Detection: Single threshold={self.MASTER_THRESHOLD}, Weights={self.weights}")
        
    def match_template(self, scene_rgb: np.ndarray, scene_depth: np.ndarray,
                      scene_ir: Optional[np.ndarray], scene_pointcloud: Optional[np.ndarray],
                      template_features: Dict) -> Tuple[float, Dict]:
        """Match scene against template using weighted fusion"""
        
        # Simple template matching for detection (fast)
        try:
            # Resize scene to match template size
            template_h, template_w = 50, 50  # Fixed size for speed
            scene_resized = cv2.resize(scene_rgb, (template_w, template_h))
            
            # Simple correlation
            correlation = cv2.matchTemplate(
                cv2.cvtColor(scene_resized, cv2.COLOR_BGR2GRAY),
                cv2.cvtColor(cv2.resize(scene_rgb, (template_h, template_w)), cv2.COLOR_BGR2GRAY),
                cv2.TM_CCOEFF_NORMED
            )
            confidence = float(correlation[0, 0])
            return max(0.0, confidence), {}
        except:
            return 0.0, {}
        
        # Compare features and calculate weighted confidence
        feature_confidences = {}
        total_weighted_confidence = 0.0
        total_weight = 0.0
        
        for feature_name, weight in self.feature_extractor.feature_weights.items():
            if feature_name in template_features and feature_name in scene_features:
                confidence = self._compare_feature(
                    template_features[feature_name], 
                    scene_features[feature_name], 
                    feature_name
                )
                
                feature_confidences[feature_name] = confidence
                total_weighted_confidence += confidence * weight
                total_weight += weight
        
        # Normalize confidence
        final_confidence = total_weighted_confidence / total_weight if total_weight > 0 else 0.0
        
        return final_confidence, feature_confidences
    
    def _compare_feature(self, template_feature, scene_feature, feature_type: str) -> float:
        """Compare individual features and return confidence"""
        
        try:
            if feature_type == 'color_histogram':
                return self._compare_histograms(template_feature, scene_feature)
            
            elif feature_type == 'edge_density':
                return self._compare_edge_features(template_feature, scene_feature)
            
            elif feature_type == 'texture_lbp':
                return self._compare_arrays(template_feature, scene_feature)
            
            elif feature_type == 'aspect_ratio':
                return self._compare_geometric_properties(template_feature, scene_feature)
            
            elif feature_type == 'contour_moments':
                return self._compare_arrays(template_feature, scene_feature)
            
            elif feature_type == 'depth_histogram':
                return self._compare_arrays(template_feature, scene_feature)
            
            elif feature_type == 'surface_curvature':
                return self._compare_curvature(template_feature, scene_feature)
            
            elif feature_type == 'depth_variance':
                return self._compare_depth_stats(template_feature, scene_feature)
            
            elif feature_type == 'volume_estimate':
                return self._compare_volume(template_feature, scene_feature)
            
            elif feature_type == 'shape_descriptor':
                return self._compare_shape_descriptor(template_feature, scene_feature)
            
            elif feature_type == 'surface_normals':
                return self._compare_surface_normals(template_feature, scene_feature)
            
            elif feature_type == 'geometric_primitives':
                return self._compare_geometric_primitives(template_feature, scene_feature)
            
            elif feature_type == 'ir_signature':
                return self._compare_ir_signature(template_feature, scene_feature)
            
            elif feature_type == 'material_properties':
                return self._compare_material_properties(template_feature, scene_feature)
            
            else:
                return 0.0
                
        except Exception as e:
            print(f"Error comparing {feature_type}: {e}")
            return 0.0
    
    def _compare_histograms(self, template_hist, scene_hist) -> float:
        """Compare histogram features"""
        correlations = []
        
        for key in ['hue', 'saturation', 'value']:
            if key in template_hist and key in scene_hist:
                corr = cv2.compareHist(
                    template_hist[key].astype(np.float32), 
                    scene_hist[key].astype(np.float32), 
                    cv2.HISTCMP_CORREL
                )
                correlations.append(max(0, corr))
        
        return np.mean(correlations) if correlations else 0.0
    
    def _compare_arrays(self, template_arr, scene_arr) -> float:
        """Compare array features using correlation"""
        try:
            template_arr = np.array(template_arr).flatten()
            scene_arr = np.array(scene_arr).flatten()
            
            if len(template_arr) != len(scene_arr):
                return 0.0
            
            # Normalize arrays
            template_norm = template_arr / (np.linalg.norm(template_arr) + 1e-7)
            scene_norm = scene_arr / (np.linalg.norm(scene_arr) + 1e-7)
            
            # Calculate correlation
            correlation = np.dot(template_norm, scene_norm)
            return max(0, correlation)
            
        except:
            return 0.0
    
    def _compare_edge_features(self, template_edge, scene_edge) -> float:
        """Compare edge features"""
        density_diff = abs(template_edge['density'] - scene_edge['density'])
        density_score = 1.0 / (1.0 + density_diff * 10)
        
        orientation_score = self._compare_arrays(
            template_edge['orientation_hist'], 
            scene_edge['orientation_hist']
        )
        
        return 0.5 * density_score + 0.5 * orientation_score
    
    def _compare_geometric_properties(self, template_geo, scene_geo) -> float:
        """Compare geometric properties"""
        scores = []
        
        for key in ['width_height_ratio', 'area_bbox_ratio', 'compactness']:
            if key in template_geo and key in scene_geo:
                diff = abs(template_geo[key] - scene_geo[key])
                score = 1.0 / (1.0 + diff * 2)
                scores.append(score)
        
        return np.mean(scores) if scores else 0.0
    
    def _compare_curvature(self, template_curv, scene_curv) -> float:
        """Compare surface curvature features"""
        if 'curvature_hist' in template_curv and 'curvature_hist' in scene_curv:
            return self._compare_arrays(template_curv['curvature_hist'], scene_curv['curvature_hist'])
        return 0.0
    
    def _compare_depth_stats(self, template_depth, scene_depth) -> float:
        """Compare depth statistics"""
        scores = []
        
        for key in ['variance', 'std', 'range']:
            if key in template_depth and key in scene_depth:
                # Relative difference
                t_val = template_depth[key]
                s_val = scene_depth[key]
                if max(t_val, s_val) > 0:
                    rel_diff = abs(t_val - s_val) / max(t_val, s_val)
                    score = 1.0 / (1.0 + rel_diff)
                    scores.append(score)
        
        return np.mean(scores) if scores else 0.0
    
    def _compare_volume(self, template_vol, scene_vol) -> float:
        """Compare volume estimates"""
        if 'volume_density' in template_vol and 'volume_density' in scene_vol:
            t_density = template_vol['volume_density']
            s_density = scene_vol['volume_density']
            
            if max(t_density, s_density) > 0:
                rel_diff = abs(t_density - s_density) / max(t_density, s_density)
                return 1.0 / (1.0 + rel_diff)
        
        return 0.0
    
    def _compare_shape_descriptor(self, template_shape, scene_shape) -> float:
        """Compare MSHD (Multi-Statistics Histogram Descriptors)"""
        if 'mshd_descriptor' in template_shape and 'mshd_descriptor' in scene_shape:
            # MSHD comparison using histogram correlation
            t_mshd = np.array(template_shape['mshd_descriptor'])
            s_mshd = np.array(scene_shape['mshd_descriptor'])
            
            # Multi-scale MSHD comparison for robustness
            confidences = []
            
            # 1. Direct MSHD correlation
            if len(t_mshd) == len(s_mshd) and len(t_mshd) > 0:
                # Normalized cross-correlation
                correlation = np.corrcoef(t_mshd, s_mshd)[0, 1]
                if not np.isnan(correlation):
                    confidences.append(max(0, correlation))
            
            # 2. Chi-squared distance (good for histograms)
            chi_squared = 0
            for i in range(min(len(t_mshd), len(s_mshd))):
                if t_mshd[i] + s_mshd[i] > 0:
                    chi_squared += ((t_mshd[i] - s_mshd[i]) ** 2) / (t_mshd[i] + s_mshd[i])
            chi_score = 1.0 / (1.0 + chi_squared / 100.0)  # Normalize
            confidences.append(chi_score)
            
            # 3. Bhattacharyya distance (histogram similarity)
            if np.sum(t_mshd) > 0 and np.sum(s_mshd) > 0:
                t_norm = t_mshd / np.sum(t_mshd)
                s_norm = s_mshd / np.sum(s_mshd)
                bhatt_coeff = np.sum(np.sqrt(t_norm * s_norm))
                bhatt_distance = -np.log(bhatt_coeff + 1e-10)
                bhatt_score = np.exp(-bhatt_distance / 2)
                confidences.append(bhatt_score)
            
            # 4. Scale-invariant geometric comparison
            if 'bbox_dimensions' in template_shape and 'bbox_dimensions' in scene_shape:
                t_dims = np.array(template_shape['bbox_dimensions'])
                s_dims = np.array(scene_shape['bbox_dimensions'])
                
                # Scale-invariant aspect ratio comparison
                if np.max(t_dims) > 0 and np.max(s_dims) > 0:
                    t_ratios = t_dims / np.max(t_dims)
                    s_ratios = s_dims / np.max(s_dims)
                    
                    diff = np.linalg.norm(t_ratios - s_ratios)
                    ratio_score = 1.0 / (1.0 + diff * 2)
                    confidences.append(ratio_score)
            
            # Weighted combination for robustness
            if confidences:
                weights = [0.4, 0.3, 0.2, 0.1][:len(confidences)]
                weights = np.array(weights) / np.sum(weights)
                return float(np.sum(np.array(confidences) * weights))
        
        # Fallback to basic comparison
        if 'bbox_dimensions' in template_shape and 'bbox_dimensions' in scene_shape:
            t_dims = np.array(template_shape['bbox_dimensions'])
            s_dims = np.array(scene_shape['bbox_dimensions'])
            
            if np.max(t_dims) > 0 and np.max(s_dims) > 0:
                t_ratios = t_dims / np.max(t_dims)
                s_ratios = s_dims / np.max(s_dims)
                
                diff = np.linalg.norm(t_ratios - s_ratios)
                return 1.0 / (1.0 + diff)
        
        return 0.0
    
    def _compare_surface_normals(self, template_normals, scene_normals) -> float:
        """Compare surface normal features"""
        if 'surface_smoothness' in template_normals and 'surface_smoothness' in scene_normals:
            diff = abs(template_normals['surface_smoothness'] - scene_normals['surface_smoothness'])
            return 1.0 / (1.0 + diff * 5)
        return 0.0
    
    def _compare_geometric_primitives(self, template_prims, scene_prims) -> float:
        """Compare geometric primitive fitness scores"""
        scores = []
        
        for primitive in ['plane_fitness', 'sphere_fitness', 'cylinder_fitness']:
            if primitive in template_prims and primitive in scene_prims:
                diff = abs(template_prims[primitive] - scene_prims[primitive])
                score = 1.0 / (1.0 + diff * 3)
                scores.append(score)
        
        return np.mean(scores) if scores else 0.0
    
    def _compare_ir_signature(self, template_ir, scene_ir) -> float:
        """Compare IR signatures"""
        hist_score = self._compare_arrays(
            template_ir['intensity_hist'], 
            scene_ir['intensity_hist']
        )
        
        intensity_diff = abs(template_ir['mean_intensity'] - scene_ir['mean_intensity']) / 255.0
        intensity_score = 1.0 / (1.0 + intensity_diff * 5)
        
        return 0.7 * hist_score + 0.3 * intensity_score
    
    def _compare_material_properties(self, template_mat, scene_mat) -> float:
        """Compare material properties"""
        scores = []
        
        for prop in ['surface_roughness', 'reflectivity_estimate', 'thermal_uniformity']:
            if prop in template_mat and prop in scene_mat:
                diff = abs(template_mat[prop] - scene_mat[prop])
                score = 1.0 / (1.0 + diff * 2)
                scores.append(score)
        
        return np.mean(scores) if scores else 0.0


class RobustMultiModalDetector:
    """
    Main Multi-Modal Object Detection and 6D Pose Estimation System
    
    Complete industrial-grade detection system featuring:
    - Intel RealSense D415 camera integration
    - 9-modal detection (RGB, IR, Depth, SIFT, AKAZE, KAZE, Contours, Edges, Features)
    - Real-time 6D pose estimation with millimeter precision
    - Template creation and persistence
    - Weighted fusion with single master threshold
    - False positive prevention and validation
    
    Usage:
        detector = RobustMultiModalDetector()
        detector.run()  # Start detection loop
        
    Controls:
        - 't': Create new template by selecting ROI
        - 'ESC': Exit application
        
    @class RobustMultiModalDetector
    @author Seifeddin Shamseldin
    @version 3.0
    @requires Intel RealSense D415 camera
    """
    
    def __init__(self):
        # Initialize camera
        self.pipeline = None
        self.align = None
        self.depth_scale = None
        self.use_realsense = REALSENSE_AVAILABLE
        
        # Initialize feature system
        self.feature_extractor = ComprehensiveFeatureExtractor()
        self.matcher = WeightedFusionMatcher(self.feature_extractor)
        
        # Initialize 6D pose estimation
        self.pose_estimator = SixDOFPoseEstimator()
        
        # Template storage
        self.templates = {}
        self.template_features = {}
        
        # Background model for validation
        self.background_depth = None
        self.background_rgb = None
        self.frame_count = 0
        
        # Performance monitoring
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0.0
        
        # Frame skipping for performance
        self.frame_skip_counter = 0
        self.expensive_ops_interval = 3  # Only do expensive ops every 3rd frame
        
        # UI state
        self.creating_template = False
        self.template_name = ""
        self.start_point = None
        self.end_point = None
        self.current_frame = None
        
        print("üöÄ Robust Multi-Modal Detector Initialized")
        
        # Load existing templates on startup
        self._load_existing_templates()
    
    def _load_existing_templates(self):
        """Load all existing templates from disk"""
        template_dir = Path("robust_templates")
        if not template_dir.exists():
            return
        
        loaded_count = 0
        for template_folder in template_dir.iterdir():
            if template_folder.is_dir():
                template_name = template_folder.name
                try:
                    # Load template files
                    rgb_path = template_folder / "rgb.jpg"
                    depth_path = template_folder / "depth.npy"
                    features_path = template_folder / "features.json"
                    ir_path = template_folder / "ir.jpg"
                    pointcloud_path = template_folder / "pointcloud.npy"
                    
                    if rgb_path.exists() and depth_path.exists():
                        # Load RGB and depth
                        rgb = cv2.imread(str(rgb_path))
                        depth = np.load(str(depth_path))
                        
                        # Load IR if available
                        ir = cv2.imread(str(ir_path), cv2.IMREAD_GRAYSCALE) if ir_path.exists() else None
                        
                        # Load point cloud if available
                        pointcloud = np.load(str(pointcloud_path)) if pointcloud_path.exists() else None
                        
                        # Load 6D pose template data
                        pose_centroid_path = template_path / "pose_centroid.npy"
                        pose_pointcloud_path = template_path / "pose_pointcloud.npy"
                        
                        if pose_centroid_path.exists():
                            pose_centroid = np.load(str(pose_centroid_path))
                            self.pose_estimator.template_centroids[template_name] = pose_centroid
                            print(f"üì¶ 6D pose centroid loaded: {pose_centroid}")
                        
                        if pose_pointcloud_path.exists():
                            pose_pointcloud = np.load(str(pose_pointcloud_path))
                            self.pose_estimator.template_point_clouds[template_name] = pose_pointcloud
                            print(f"üì¶ 6D pose point cloud loaded: {len(pose_pointcloud)} points")
                        
                        # Enhanced feature extraction for detection
                        sift = cv2.SIFT_create(nfeatures=500)
                        akaze = cv2.AKAZE_create()
                        kaze = cv2.KAZE_create()
                        gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)
                        sift_kp, sift_desc = sift.detectAndCompute(gray, None)
                        akaze_kp, akaze_desc = akaze.detectAndCompute(gray, None)
                        kaze_kp, kaze_desc = kaze.detectAndCompute(gray, None)
                        
                        # Store enhanced template
                        self.templates[template_name] = {
                            'rgb': rgb,
                            'depth': depth,
                            'ir': ir,
                            'pointcloud': pointcloud,
                            'roi': (0, 0, rgb.shape[1], rgb.shape[0]),  # Full image
                            'sift_kp': sift_kp,
                            'sift_desc': sift_desc,
                            'sift_count': len(sift_kp) if sift_kp else 0,
                            'akaze_kp': akaze_kp,
                            'akaze_desc': akaze_desc,
                            'akaze_count': len(akaze_kp) if akaze_kp else 0,
                            'kaze_kp': kaze_kp,
                            'kaze_desc': kaze_desc,
                            'kaze_count': len(kaze_kp) if kaze_kp else 0,
                            'main_contour': None,  # Will compute if needed
                            'edge_template': cv2.Canny(gray, 50, 150),
                        }
                        
                        # Load features if available
                        if features_path.exists():
                            with open(features_path, 'r') as f:
                                self.template_features[template_name] = json.load(f)
                        
                        loaded_count += 1
                        print(f"üìÇ Loaded template: {template_name}")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to load template {template_name}: {e}")
        
        if loaded_count > 0:
            print(f"‚úÖ Loaded {loaded_count} existing templates")
        else:
            print("üìù No existing templates found")
        
    def initialize_camera(self) -> bool:
        """Initialize D415 camera"""
        if not self.use_realsense:
            print("‚ùå RealSense not available")
            return False
            
        try:
            self.pipeline = rs.pipeline()
            config = rs.config()
            
            # D415 Optimal resolution configurations (based on Intel specs)
            resolutions = [
                (1280, 720),  # Maximum depth resolution for D415
                (848, 480),   # Optimal balanced resolution (recommended)
                (640, 480),   # Standard resolution
                (424, 240),   # Lower resolution fallback
            ]
            
            pipeline_started = False
            for width, height in resolutions:
                try:
                    config = rs.config()  # Reset config
                    
                    # D415 supports higher RGB resolution than depth
                    if width == 1280:
                        # Use maximum RGB resolution with maximum depth
                        config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)
                        config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
                        config.enable_stream(rs.stream.infrared, 1, 1280, 720, rs.format.y8, 30)
                    else:
                        # Matched resolutions for other settings
                        config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, 30)
                        config.enable_stream(rs.stream.depth, width, height, rs.format.z16, 30)
                        config.enable_stream(rs.stream.infrared, 1, width, height, rs.format.y8, 30)
                    
                    profile = self.pipeline.start(config)
                    pipeline_started = True
                    print(f"‚úÖ Camera started with resolution: {width}x{height}")
                    break
                except Exception as e:
                    print(f"‚ö†Ô∏è  Resolution {width}x{height} failed: {e}")
                    if self.pipeline:
                        try:
                            self.pipeline.stop()
                        except:
                            pass
                    continue
            
            if not pipeline_started:
                raise Exception("All resolution configurations failed")
            
            # Get depth scale and optimize D415 settings
            depth_sensor = profile.get_device().first_depth_sensor()
            self.depth_scale = depth_sensor.get_depth_scale()
            
            # D415 Performance Optimizations
            try:
                # Set visual preset for better accuracy (D415 optimized)
                depth_sensor.set_option(rs.option.visual_preset, 4)  # High Accuracy preset
                
                # Optimize exposure and gain for industrial use
                depth_sensor.set_option(rs.option.enable_auto_exposure, 1)
                
                # Set laser power for optimal range (D415 specific)
                depth_sensor.set_option(rs.option.laser_power, 240)  # High laser power
                
                # Improve accuracy with error correction
                depth_sensor.set_option(rs.option.accuracy, 3)  # Maximum accuracy
                
                # Motion vs accuracy tradeoff (favor accuracy for industrial use)
                depth_sensor.set_option(rs.option.motion_range, 1)  # Low motion range for higher accuracy
                
                # Confidence threshold for better depth quality
                depth_sensor.set_option(rs.option.confidence_threshold, 2)  # Medium confidence
                
                print("‚úÖ D415 performance optimizations applied")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Some D415 optimizations not available: {e}")
            
            # Optimize RGB camera settings for D415
            try:
                rgb_sensor = profile.get_device().query_sensors()[1]  # RGB sensor
                rgb_sensor.set_option(rs.option.enable_auto_exposure, 1)
                rgb_sensor.set_option(rs.option.enable_auto_white_balance, 1)
                # Set higher saturation and contrast for better template matching
                if rgb_sensor.supports(rs.option.saturation):
                    rgb_sensor.set_option(rs.option.saturation, 64)  # Higher saturation
                if rgb_sensor.supports(rs.option.contrast):
                    rgb_sensor.set_option(rs.option.contrast, 50)   # Good contrast
                print("‚úÖ RGB camera optimizations applied")
            except Exception as e:
                print(f"‚ö†Ô∏è  RGB optimizations not available: {e}")
            
            # Try to set up filters for better depth quality (optional)
            try:
                # Enable hole filling for better depth data
                hole_filling = rs.hole_filling_filter()
                hole_filling.set_option(rs.option.holes_fill, 2)  # Fill all holes
                
                # Spatial filter for noise reduction
                spatial = rs.spatial_filter()
                spatial.set_option(rs.option.filter_magnitude, 2)
                spatial.set_option(rs.option.filter_smooth_alpha, 0.5)
                spatial.set_option(rs.option.filter_smooth_delta, 20)
                
                # Temporal filter for stable tracking
                temporal = rs.temporal_filter()
                temporal.set_option(rs.option.filter_smooth_alpha, 0.4)
                temporal.set_option(rs.option.filter_smooth_delta, 20)
                
                # Store filters
                self.hole_filling = hole_filling
                self.spatial_filter = spatial
                self.temporal_filter = temporal
                self.filters_available = True
                print("‚úÖ Depth filters enabled")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Depth filters not available: {e}")
                self.hole_filling = None
                self.spatial_filter = None
                self.temporal_filter = None
                self.filters_available = False
            
            # Create alignment object
            self.align = rs.align(rs.stream.color)
            
            print("‚úÖ D415 Camera initialized with RGB + Depth + IR")
            return True
            
        except Exception as e:
            print(f"‚ùå Camera initialization failed: {e}")
            return False
    
    def get_frames(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:
        """Get RGB, Depth, and IR frames"""
        try:
            frames = self.pipeline.wait_for_frames()
            aligned_frames = self.align.process(frames)
            
            # Get frames
            color_frame = aligned_frames.get_color_frame()
            depth_frame = aligned_frames.get_depth_frame()
            ir_frame = frames.get_infrared_frame(1)
            
            if not color_frame or not depth_frame:
                return None, None, None
            
            # Apply filters to depth for better quality and range (if available)
            if self.filters_available:
                try:
                    depth_frame = self.spatial_filter.process(depth_frame)
                    depth_frame = self.temporal_filter.process(depth_frame)
                    depth_frame = self.hole_filling.process(depth_frame)
                except:
                    pass  # Continue without filters if they fail
            
            # Convert to numpy arrays
            rgb = np.asanyarray(color_frame.get_data())
            depth = np.asanyarray(depth_frame.get_data())
            ir = np.asanyarray(ir_frame.get_data()) if ir_frame else None
            
            return rgb, depth, ir
            
        except Exception as e:
            print(f"Frame capture error: {e}")
            return None, None, None
    
    def depth_to_pointcloud(self, depth: np.ndarray, roi: Tuple[int, int, int, int]) -> Optional[np.ndarray]:
        """Convert depth ROI to point cloud"""
        try:
            x, y, w, h = roi
            
            # Validate ROI bounds
            if y + h > depth.shape[0] or x + w > depth.shape[1] or w <= 0 or h <= 0:
                return None
                
            depth_roi = depth[y:y+h, x:x+w]
            
            # Check if depth ROI has valid data
            if depth_roi.size == 0 or np.all(depth_roi == 0):
                return None
            
            # Get depth intrinsics
            profile = self.pipeline.get_active_profile()
            depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))
            depth_intrinsics = depth_profile.get_intrinsics()
            
            # Create point cloud with bounds checking
            points = []
            for v in range(min(h, depth_roi.shape[0])):
                for u in range(min(w, depth_roi.shape[1])):
                    if v < depth_roi.shape[0] and u < depth_roi.shape[1]:
                        d = depth_roi[v, u] * self.depth_scale
                        if d > 0.1 and d < 5.0:  # Valid depth range: 10cm to 5m
                            # Convert to 3D coordinates
                            x_3d = (u + x - depth_intrinsics.ppx) * d / depth_intrinsics.fx
                            y_3d = (v + y - depth_intrinsics.ppy) * d / depth_intrinsics.fy
                            z_3d = d
                            points.append([x_3d, y_3d, z_3d])
            
            # Return point cloud only if we have sufficient points
            if len(points) >= 10:  # Minimum points for meaningful pose estimation
                return np.array(points)
            else:
                return None
            
        except Exception as e:
            print(f"Point cloud conversion error: {e}")
            return None
    
    def create_template(self, name: str, rgb: np.ndarray, depth: np.ndarray, 
                       ir: Optional[np.ndarray], roi: Tuple[int, int, int, int]):
        """Create comprehensive template with all features"""
        # Safety check to prevent duplicate creation
        if name in self.templates:
            print(f"‚ö†Ô∏è  Template {name} already exists!")
            return
            
        x, y, w, h = roi
        
        # Extract ROIs
        rgb_roi = rgb[y:y+h, x:x+w]
        depth_roi = depth[y:y+h, x:x+w]
        ir_roi = ir[y:y+h, x:x+w] if ir is not None else None
        
        # Generate point cloud
        pointcloud = self.depth_to_pointcloud(depth, roi)
        
        print(f"\nüèóÔ∏è  Creating simple template: {name}")
        print(f"   ROI: {roi}")
        print(f"   Point cloud points: {len(pointcloud) if pointcloud is not None else 0}")
        
        # Extract comprehensive features (call only once)
        print("üîß Starting feature extraction...")
        features = self.feature_extractor.extract_template_features(
            rgb_roi, depth_roi, ir_roi, pointcloud
        )
        print("üîß Feature extraction completed!")
        
        # Extract SIFT features for robust detection
        sift = cv2.SIFT_create(nfeatures=500)
        gray_roi = cv2.cvtColor(rgb_roi, cv2.COLOR_BGR2GRAY)
        
        # Skip SURF - causing crashes
        surf_kp, surf_desc = [], None
            
        sift_kp, sift_desc = sift.detectAndCompute(gray_roi, None)
        
        # Extract AKAZE features (better than ORB)
        akaze = cv2.AKAZE_create()
        akaze_kp, akaze_desc = akaze.detectAndCompute(gray_roi, None)
        
        # Extract KAZE features (research shows best performance)
        kaze = cv2.KAZE_create()
        kaze_kp, kaze_desc = kaze.detectAndCompute(gray_roi, None)
        
        # Extract contour shape features
        contours, _ = cv2.findContours(cv2.Canny(gray_roi, 50, 150), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        main_contour = max(contours, key=cv2.contourArea) if contours else None
        
        print(f"üîç SIFT: {len(sift_kp) if sift_kp else 0} keypoints")
        print(f"üîç AKAZE: {len(akaze_kp) if akaze_kp else 0} keypoints (UPGRADED from ORB)")
        print(f"üîç KAZE: {len(kaze_kp) if kaze_kp else 0} keypoints (BEST traditional)")
        print(f"üîç Contours: {len(contours) if contours else 0} found")
        
        # Store ENHANCED template with upgraded algorithms
        self.templates[name] = {
            'rgb': rgb_roi,           # Simple RGB ROI
            'depth': depth_roi,       # Simple depth ROI
            'ir': ir_roi,            # Simple IR ROI
            'pointcloud': pointcloud,   # Point cloud
            'roi': roi,
            'surf_kp': surf_kp,       # NEW - Better for far objects
            'surf_desc': surf_desc,
            'surf_count': len(surf_kp) if surf_kp else 0,
            'sift_kp': sift_kp,
            'sift_desc': sift_desc,
            'sift_count': len(sift_kp) if sift_kp else 0,
            'akaze_kp': akaze_kp,     # UPGRADED from ORB
            'akaze_desc': akaze_desc,
            'akaze_count': len(akaze_kp) if akaze_kp else 0,
            'kaze_kp': kaze_kp,       # NEW - Best traditional algorithm
            'kaze_desc': kaze_desc,
            'kaze_count': len(kaze_kp) if kaze_kp else 0,
            'main_contour': main_contour,
            'edge_template': cv2.Canny(gray_roi, 50, 150),  # Simple edges
        }
        
        self.template_features[name] = features
        
        # Add point cloud to 6D pose estimator
        if pointcloud is not None:
            print(f"ü§ñ Adding {len(pointcloud)} points to 6D pose estimator...")
            success = self.pose_estimator.add_template_pointcloud(name, pointcloud)
            if success:
                print(f"‚úÖ 6D pose template '{name}' stored successfully")
            else:
                print(f"‚ùå Failed to store 6D pose template '{name}'")
        else:
            print("‚ö†Ô∏è  No point cloud available for 6D pose estimation")
        
        print(f"‚úÖ Template '{name}' created with {len(features)} feature types")
        
        # Save to disk
        self._save_template(name)
        
        print(f"üíæ Template '{name}' saved! Ready for detection.")
        
        # Force exit template creation mode
        self.creating_template = False
        self.start_point = None
        self.end_point = None
        print("üîÑ Switched to DETECTION MODE")
    

    def _save_template(self, name: str):
        """Save template and features to disk"""
        template_dir = Path("robust_templates")
        template_dir.mkdir(exist_ok=True)
        
        template_path = template_dir / name
        template_path.mkdir(exist_ok=True)
        
        template = self.templates[name]
        
        # Save images and data
        cv2.imwrite(str(template_path / "rgb.jpg"), template['rgb'])
        np.save(str(template_path / "depth.npy"), template['depth'])
        
        if template['ir'] is not None:
            cv2.imwrite(str(template_path / "ir.jpg"), template['ir'])
        
        if template['pointcloud'] is not None:
            np.save(str(template_path / "pointcloud.npy"), template['pointcloud'])
        
        # Save 6D pose template data
        if name in self.pose_estimator.template_centroids:
            centroid = self.pose_estimator.template_centroids[name]
            np.save(str(template_path / "pose_centroid.npy"), centroid)
            print(f"üíæ 6D pose centroid saved: {centroid}")
        
        if name in self.pose_estimator.template_point_clouds:
            pose_points = self.pose_estimator.template_point_clouds[name]
            np.save(str(template_path / "pose_pointcloud.npy"), pose_points)
            print(f"üíæ 6D pose point cloud saved: {len(pose_points)} points")
        
        # Save features as JSON (convert numpy arrays to lists)
        features_serializable = self._make_serializable(self.template_features[name])
        with open(template_path / "features.json", 'w') as f:
            json.dump(features_serializable, f, indent=2)
        
        print(f"üíæ Template saved to {template_path}")
    
    def _make_serializable(self, obj):
        """Convert numpy arrays and types to lists/primitives for JSON serialization"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating, np.uint16, np.uint8, np.int32, np.float32, np.float64)):
            return obj.item()  # Convert numpy scalar to Python primitive
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj
    
    def _validate_templates(self):
        """Remove templates whose files no longer exist on disk"""
        template_dir = Path("robust_templates")
        templates_to_remove = []
        
        for template_name in self.templates.keys():
            template_path = template_dir / template_name
            if not template_path.exists():
                templates_to_remove.append(template_name)
                print(f"üóëÔ∏è  Removed template '{template_name}' - files no longer exist")
        
        for template_name in templates_to_remove:
            del self.templates[template_name]
            if template_name in self.template_features:
                del self.template_features[template_name]
    
    def _update_background_model(self, rgb: np.ndarray, depth: np.ndarray):
        """Update background model for object presence validation"""
        self.frame_count += 1
        
        # Build background model from first 30 frames (without objects)
        if self.frame_count <= 30:
            if self.background_depth is None:
                self.background_depth = depth.astype(np.float32)
                self.background_rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32)
            else:
                # Running average
                alpha = 0.1
                self.background_depth = (1 - alpha) * self.background_depth + alpha * depth.astype(np.float32)
                gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32)
                self.background_rgb = (1 - alpha) * self.background_rgb + alpha * gray
    
    def _has_significant_object(self, rgb: np.ndarray, depth: np.ndarray, location: Tuple[int, int, int, int]) -> bool:
        """Check if there's a significant object at the given location"""
        if self.background_depth is None or self.background_rgb is None:
            return True  # No background model yet, assume object present
        
        x, y, w, h = location
        if y + h >= depth.shape[0] or x + w >= depth.shape[1]:
            return False  # Invalid location
        
        # Extract ROI
        scene_depth_roi = depth[y:y+h, x:x+w]
        bg_depth_roi = self.background_depth[y:y+h, x:x+w]
        
        scene_gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)
        scene_rgb_roi = scene_gray[y:y+h, x:x+w]
        bg_rgb_roi = self.background_rgb[y:y+h, x:x+w]
        
        # Balanced background detection for industrial use
        depth_diff = np.abs(scene_depth_roi.astype(np.float32) - bg_depth_roi)
        valid_depth_pixels = scene_depth_roi > 0
        
        # Check for meaningful depth change
        if np.sum(valid_depth_pixels) > 20:
            depth_change_mean = np.mean(depth_diff[valid_depth_pixels])
            # Object should create noticeable depth change
            significant_depth_change = depth_change_mean > 40  # 4cm change
        else:
            significant_depth_change = False
        
        # Check for meaningful RGB change
        rgb_diff = np.abs(scene_rgb_roi.astype(np.float32) - bg_rgb_roi)
        rgb_change_mean = np.mean(rgb_diff)
        # Object should create noticeable color change
        significant_rgb_change = rgb_change_mean > 15  # Reasonable color change
        
        # Either depth OR RGB change indicates object presence
        return significant_depth_change or significant_rgb_change
    
    def detect_objects(self, rgb: np.ndarray, depth: np.ndarray, ir: Optional[np.ndarray]) -> List[Dict]:
        """Detect objects using comprehensive feature matching"""
        detections = []
        
        # Update background model
        self._update_background_model(rgb, depth)
        
        # Validate templates still exist on disk
        self._validate_templates()
        
        if not self.templates:
            return detections
        
        # ULTIMATE 9-MODALITY WEIGHTED FUSION DETECTION
        gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)
        
        # Enhanced feature extractors - UPGRADED FROM ORB
        sift = cv2.SIFT_create(nfeatures=500)
        akaze = cv2.AKAZE_create()  # AKAZE - Better than ORB for object detection
        kaze = cv2.KAZE_create()    # KAZE - Research shows best traditional performance
        
        # PERFORMANCE: Skip expensive features every few frames  
        self.frame_skip_counter += 1
        do_full_features = (self.frame_skip_counter % 2 == 0)  # Every 2nd frame
        
        # Skip SURF for now - causing issues
        scene_surf_kp, scene_surf_desc = [], None
        
        # SIFT as backup (most reliable for close objects)
        scene_sift_kp, scene_sift_desc = sift.detectAndCompute(gray, None)
        
        if do_full_features:
            # Only compute expensive features every 2nd frame
            scene_akaze_kp, scene_akaze_desc = akaze.detectAndCompute(gray, None)
            scene_kaze_kp, scene_kaze_desc = kaze.detectAndCompute(gray, None)
        else:
            # Use dummy values to skip computation
            scene_akaze_kp, scene_akaze_desc = [], None
            scene_kaze_kp, scene_kaze_desc = [], None
        
        # Edge map for edge-based matching
        scene_edges = cv2.Canny(gray, 50, 150)
        
        # Scene contours for shape matching
        scene_contours, _ = cv2.findContours(scene_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Optimized industrial tolerance based on performance analysis
        GLOBAL_TOLERANCE = 0.55  # Lowered for far object detection (2+ meters)
        
        for template_name, template_data in self.templates.items():
            # === EXTRACT ALL MODALITY CONFIDENCES ===
            confidences = {}
            locations = {}
            
            # 1. ARRCH RGB Template Matching (Adaptive Radial Ring Code Histograms - 2024 Algorithm)
            template_gray = cv2.cvtColor(template_data['rgb'], cv2.COLOR_BGR2GRAY)
            h, w = template_gray.shape
            
            # ADAPTIVE ARRCH: More scales when needed for far objects
            # First try fast approach
            fast_scales = [1.0, 1.2]
            best_confidence = 0.0
            best_location = (0, 0)
            best_h, best_w = h, w
            
            for scale in fast_scales:
                try:
                    if scale != 1.0:
                        new_h, new_w = int(h * scale), int(w * scale)
                        if new_h > 10 and new_w > 10 and new_h < gray.shape[0] and new_w < gray.shape[1]:
                            scaled_template = cv2.resize(template_gray, (new_w, new_h))
                            scale_conf, scale_loc = self._arrch_template_match_fast(scaled_template, gray)
                            if scale_conf > best_confidence:
                                best_confidence = scale_conf
                                best_location = scale_loc
                                best_h, best_w = new_h, new_w
                    else:
                        scale_conf, scale_loc = self._arrch_template_match_fast(template_gray, gray)
                        if scale_conf > best_confidence:
                            best_confidence = scale_conf
                            best_location = scale_loc
                except:
                    continue
            
            # FAR OBJECT: Try many small scales for 2+ meter detection
            if best_confidence < 0.8:  # More aggressive threshold
                far_scales = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45]  # Small scales for far objects
                for scale in far_scales:
                    try:
                        new_h, new_w = int(h * scale), int(w * scale)
                        if new_h > 10 and new_w > 10 and new_h < gray.shape[0] and new_w < gray.shape[1]:
                            scaled_template = cv2.resize(template_gray, (new_w, new_h))
                            result = cv2.matchTemplate(gray, scaled_template, cv2.TM_CCOEFF_NORMED)
                            _, max_val, _, max_loc = cv2.minMaxLoc(result)
                            if max_val > best_confidence:
                                best_confidence = max_val
                                best_location = max_loc
                                best_h, best_w = new_h, new_w
                    except:
                        continue
            
            # Best scale confidence
            confidences['rgb_template'] = float(best_confidence)
            locations['rgb_template'] = (best_location[0], best_location[1], best_w, best_h)
            
            # 2. SURF + SIFT Feature Matching (SURF better for far objects)
            confidences['surf'] = 0.0  
            confidences['sift'] = 0.0
            locations['surf'] = locations['rgb_template']  # fallback
            locations['sift'] = locations['rgb_template']  # fallback
            
            # SURF matching for far distance (2+ meters)
            if (template_data.get('surf_desc') is not None and scene_surf_desc is not None):
                try:
                    # SURF uses L2 norm
                    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
                    surf_matches = bf.knnMatch(template_data['surf_desc'], scene_surf_desc, k=2)
                    
                    # Lowe's ratio test
                    good_surf_matches = []
                    for match_pair in surf_matches:
                        if len(match_pair) == 2:
                            m, n = match_pair
                            if m.distance < 0.7 * n.distance:  # More permissive for far objects
                                good_surf_matches.append(m)
                    
                    if len(good_surf_matches) >= 2:
                        template_surf_count = max(template_data.get('surf_count', 1), 1)
                        surf_ratio = min(len(good_surf_matches) / template_surf_count, 1.0)
                        confidences['surf'] = min(1.0, surf_ratio * 1.5)  # SURF boost for far objects
                        
                except:
                    pass
            
            # SIFT matching (close objects backup)
            if (template_data.get('sift_desc') is not None and scene_sift_desc is not None):
                try:
                    # FLANN with optimized parameters
                    FLANN_INDEX_KDTREE = 1
                    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=10)  # More trees for accuracy
                    search_params = dict(checks=100)  # More checks for accuracy
                    flann = cv2.FlannBasedMatcher(index_params, search_params)
                    matches = flann.knnMatch(template_data['sift_desc'], scene_sift_desc, k=2)
                    
                    # Enhanced ratio test with adaptive threshold
                    good_matches = []
                    for match_pair in matches:
                        if len(match_pair) == 2:
                            m, n = match_pair
                            # Adaptive ratio based on descriptor distance
                            ratio_threshold = 0.6 if m.distance < 0.2 else 0.75
                            if m.distance < ratio_threshold * n.distance:
                                good_matches.append(m)
                    
                    if len(good_matches) >= 4:
                        template_kp = template_data['sift_kp']
                        src_pts = np.float32([template_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
                        dst_pts = np.float32([scene_sift_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
                        
                        # Robust homography with multiple methods
                        methods = [cv2.RANSAC, cv2.LMEDS, cv2.RHO]
                        best_inliers = 0
                        best_homography = None
                        
                        for method in methods:
                            M, mask = cv2.findHomography(src_pts, dst_pts, method, 5.0)
                            if M is not None and mask is not None:
                                inliers = np.sum(mask)
                                if inliers > best_inliers:
                                    best_inliers = inliers
                                    best_homography = M
                        
                        if best_homography is not None and best_inliers >= 4:
                            # Calculate geometric consistency score
                            inlier_ratio = best_inliers / len(good_matches)
                            match_density = len(good_matches) / template_data.get('sift_count', 1)
                            
                            # Advanced confidence calculation
                            geometric_score = inlier_ratio * 0.7 + match_density * 0.3
                            confidences['sift'] = min(1.0, geometric_score * 2.0)
                            
                            # Get precise location from homography
                            corners = np.float32([[0,0],[w,0],[w,h],[0,h]]).reshape(-1,1,2)
                            transformed = cv2.perspectiveTransform(corners, best_homography)
                            x_coords, y_coords = transformed[:, 0, 0], transformed[:, 0, 1]
                            
                            # Validate transformed corners for reasonable shape
                            area = cv2.contourArea(transformed)
                            template_area = w * h
                            if 0.1 * template_area < area < 10 * template_area:  # Reasonable scale change
                                locations['sift'] = (int(min(x_coords)), int(min(y_coords)), 
                                                   int(max(x_coords) - min(x_coords)), 
                                                   int(max(y_coords) - min(y_coords)))
                except:
                    pass
            
            # 3. ENHANCED Depth Template Matching with Preprocessing
            confidences['depth'] = 0.0
            if template_data.get('depth') is not None:
                try:
                    template_depth = template_data['depth'].copy()
                    scene_depth = depth.copy()
                    
                    # Advanced depth preprocessing
                    # 1. Remove noise with bilateral filter
                    template_depth_filtered = cv2.bilateralFilter(template_depth.astype(np.float32), 9, 75, 75)
                    scene_depth_filtered = cv2.bilateralFilter(scene_depth.astype(np.float32), 9, 75, 75)
                    
                    # 2. Edge-preserving smoothing
                    template_depth_smooth = cv2.edgePreservingFilter(template_depth_filtered.astype(np.uint8), flags=1, sigma_s=50, sigma_r=0.4)
                    scene_depth_smooth = cv2.edgePreservingFilter(scene_depth_filtered.astype(np.uint8), flags=1, sigma_s=50, sigma_r=0.4)
                    
                    # 3. Normalize with better range preservation
                    template_depth_norm = cv2.normalize(template_depth_smooth, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
                    scene_depth_norm = cv2.normalize(scene_depth_smooth, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
                    
                    # 4. Multiple template matching methods for depth
                    depth_methods = [cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR_NORMED]
                    depth_scores = []
                    
                    for method in depth_methods:
                        result = cv2.matchTemplate(scene_depth_norm, template_depth_norm, method)
                        _, max_val, _, _ = cv2.minMaxLoc(result)
                        depth_scores.append(max_val)
                    
                    # 5. Combine depth scores with geometric validation
                    raw_depth_conf = float(0.7 * depth_scores[0] + 0.3 * depth_scores[1])
                    
                    # 6. Depth gradient consistency check
                    template_grad_x = cv2.Sobel(template_depth_norm, cv2.CV_64F, 1, 0, ksize=3)
                    template_grad_y = cv2.Sobel(template_depth_norm, cv2.CV_64F, 0, 1, ksize=3)
                    template_gradient = np.sqrt(template_grad_x**2 + template_grad_y**2)
                    
                    scene_grad_x = cv2.Sobel(scene_depth_norm, cv2.CV_64F, 1, 0, ksize=3)
                    scene_grad_y = cv2.Sobel(scene_depth_norm, cv2.CV_64F, 0, 1, ksize=3)
                    scene_gradient = np.sqrt(scene_grad_x**2 + scene_grad_y**2)
                    
                    grad_result = cv2.matchTemplate(scene_gradient.astype(np.uint8), 
                                                  template_gradient.astype(np.uint8), cv2.TM_CCOEFF_NORMED)
                    _, grad_conf, _, _ = cv2.minMaxLoc(grad_result)
                    
                    # Combined depth confidence with gradient consistency
                    confidences['depth'] = float(0.8 * raw_depth_conf + 0.2 * grad_conf)
                    
                except:
                    pass
            
            # 4. RIFT IR Template Matching (Radiation Insensitive Feature Transform - 2024 Algorithm)
            confidences['ir'] = 0.0
            if template_data.get('ir') is not None and ir is not None:
                try:
                    # FAST RIFT: Optimized for real-time (reduced from 4 scales to 2)
                    ir_enhanced = self._rift_enhance_ir_fast(ir)
                    template_ir_enhanced = self._rift_enhance_ir_fast(template_data['ir'])
                    
                    # ADAPTIVE IR: Fast first, then more scales if needed
                    fast_scales = [1.0, 1.2]
                    scale_confidences = []
                    best_ir_conf = 0.0
                    
                    for scale in fast_scales:
                        try:
                            if scale != 1.0:
                                h, w = template_ir_enhanced.shape
                                new_h, new_w = int(h * scale), int(w * scale)
                                if new_h > 0 and new_w > 0 and new_h < ir_enhanced.shape[0] and new_w < ir_enhanced.shape[1]:
                                    scaled_template = cv2.resize(template_ir_enhanced, (new_w, new_h))
                                    scale_conf = self._rift_template_match_fast(scaled_template, ir_enhanced)
                                    scale_confidences.append(scale_conf)
                                    best_ir_conf = max(best_ir_conf, scale_conf)
                            else:
                                scale_conf = self._rift_template_match_fast(template_ir_enhanced, ir_enhanced)
                                scale_confidences.append(scale_conf)
                                best_ir_conf = max(best_ir_conf, scale_conf)
                        except:
                            scale_confidences.append(0.0)
                    
                    # SIMPLE IR: Just 1 extra scale if needed
                    if best_ir_conf < 0.7:
                        try:
                            h, w = template_ir_enhanced.shape
                            scaled_template = cv2.resize(template_ir_enhanced, (int(w*0.8), int(h*0.8)))
                            result = cv2.matchTemplate(ir_enhanced, scaled_template, cv2.TM_CCOEFF_NORMED)
                            _, max_val, _, _ = cv2.minMaxLoc(result)
                            best_ir_conf = max(best_ir_conf, max_val)
                        except:
                            pass
                    
                    # Best scale confidence 
                    confidences['ir'] = float(best_ir_conf)
                    
                except:
                    pass
            
            # 5. ENHANCED AKAZE Feature Matching (UPGRADED from ORB)
            confidences['akaze'] = 0.0
            locations['akaze'] = locations['rgb_template']  # fallback
            if (template_data.get('akaze_desc') is not None and scene_akaze_desc is not None):
                try:
                    # AKAZE matching (better than ORB for object detection)
                    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)  # AKAZE uses binary descriptors
                    akaze_matches = bf.knnMatch(template_data['akaze_desc'], scene_akaze_desc, k=2)
                    
                    # Apply Lowe's ratio test (optimized for AKAZE)
                    good_akaze_matches = []
                    for match_pair in akaze_matches:
                        if len(match_pair) == 2:
                            m, n = match_pair
                            # AKAZE-optimized ratio test (better than ORB)
                            if m.distance < 0.75 * n.distance and m.distance < 70:  # More strict than ORB
                                good_akaze_matches.append(m)
                        elif len(match_pair) == 1:
                            # Single match - AKAZE is more reliable
                            if match_pair[0].distance < 50:  # Stricter than ORB
                                good_akaze_matches.append(match_pair[0])
                    
                    if len(good_akaze_matches) >= 2:
                        # AKAZE confidence calculation
                        template_akaze_count = max(template_data.get('akaze_count', 1), 1)
                        match_ratio = min(len(good_akaze_matches) / template_akaze_count, 1.0)
                        
                        # Distance-based scoring (AKAZE is more accurate than ORB)
                        avg_distance = np.mean([m.distance for m in good_akaze_matches])
                        distance_score = max(0, 1.0 - avg_distance / 70.0)  # Better than ORB
                        
                        # Geometric consistency bonus (AKAZE is more reliable)
                        geometric_bonus = 1.0
                        if len(good_akaze_matches) >= 4:
                            geometric_bonus = 1.3  # Higher bonus than ORB
                        
                        confidences['akaze'] = min(1.0, match_ratio * distance_score * geometric_bonus * 1.8)  # Higher multiplier
                        
                        # Get AKAZE location via homography
                        if len(good_akaze_matches) >= 4:
                            template_akaze_kp = template_data['akaze_kp']
                            src_pts = np.float32([template_akaze_kp[m.queryIdx].pt for m in good_akaze_matches]).reshape(-1, 1, 2)
                            dst_pts = np.float32([scene_akaze_kp[m.trainIdx].pt for m in good_akaze_matches]).reshape(-1, 1, 2)
                            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
                            if M is not None:
                                corners = np.float32([[0,0],[w,0],[w,h],[0,h]]).reshape(-1,1,2)
                                transformed = cv2.perspectiveTransform(corners, M)
                                x_coords, y_coords = transformed[:, 0, 0], transformed[:, 0, 1]
                                locations['akaze'] = (int(min(x_coords)), int(min(y_coords)), 
                                                    int(max(x_coords) - min(x_coords)), 
                                                    int(max(y_coords) - min(y_coords)))
                except:
                    pass
            
            # 6. NEW KAZE Feature Matching (BEST Traditional Algorithm)
            confidences['kaze'] = 0.0
            locations['kaze'] = locations['rgb_template']  # fallback
            if (template_data.get('kaze_desc') is not None and scene_kaze_desc is not None):
                try:
                    # KAZE matching (research shows best traditional performance)
                    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)  # KAZE uses float descriptors
                    kaze_matches = bf.knnMatch(template_data['kaze_desc'], scene_kaze_desc, k=2)
                    
                    # Apply Lowe's ratio test (optimized for KAZE)
                    good_kaze_matches = []
                    for match_pair in kaze_matches:
                        if len(match_pair) == 2:
                            m, n = match_pair
                            # KAZE-optimized ratio test (like SIFT)
                            if m.distance < 0.7 * n.distance:  # Standard ratio for float descriptors
                                good_kaze_matches.append(m)
                    
                    if len(good_kaze_matches) >= 4:
                        # KAZE confidence calculation
                        template_kaze_count = max(template_data.get('kaze_count', 1), 1)
                        match_ratio = min(len(good_kaze_matches) / template_kaze_count, 1.0)
                        
                        # Distance-based scoring (KAZE research shows best performance)
                        distances = [m.distance for m in good_kaze_matches]
                        avg_distance = np.mean(distances)
                        distance_score = max(0, 1.0 - avg_distance / 0.3)  # Float descriptor normalization
                        
                        # Geometric consistency bonus (KAZE is most robust)
                        geometric_bonus = 1.0
                        if len(good_kaze_matches) >= 6:
                            geometric_bonus = 1.4  # Highest bonus - best algorithm
                        
                        confidences['kaze'] = min(1.0, match_ratio * distance_score * geometric_bonus * 2.0)  # Highest multiplier
                        
                        # Get KAZE location via homography
                        if len(good_kaze_matches) >= 4:
                            template_kaze_kp = template_data['kaze_kp']
                            src_pts = np.float32([template_kaze_kp[m.queryIdx].pt for m in good_kaze_matches]).reshape(-1, 1, 2)
                            dst_pts = np.float32([scene_kaze_kp[m.trainIdx].pt for m in good_kaze_matches]).reshape(-1, 1, 2)
                            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
                            if M is not None:
                                corners = np.float32([[0,0],[w,0],[w,h],[0,h]]).reshape(-1,1,2)
                                transformed = cv2.perspectiveTransform(corners, M)
                                x_coords, y_coords = transformed[:, 0, 0], transformed[:, 0, 1]
                                locations['kaze'] = (int(min(x_coords)), int(min(y_coords)), 
                                                   int(max(x_coords) - min(x_coords)), 
                                                   int(max(y_coords) - min(y_coords)))
                except:
                    pass
            
            # 7. ENHANCED Contour Shape Matching (FIXED)
            confidences['contour'] = 0.0
            try:
                # Extract template contours dynamically if not stored
                template_contour = template_data.get('main_contour')
                if template_contour is None:
                    # Generate template contour on-the-fly
                    template_rgb = template_data.get('rgb')
                    if template_rgb is not None:
                        template_gray = cv2.cvtColor(template_rgb, cv2.COLOR_BGR2GRAY)
                        template_edges = cv2.Canny(template_gray, 50, 150)
                        template_contours, _ = cv2.findContours(template_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        if template_contours:
                            template_contour = max(template_contours, key=cv2.contourArea)
                
                if template_contour is not None and len(scene_contours) > 0:
                    best_shape_match = float('inf')
                    best_area_match = 0.0
                    template_area = cv2.contourArea(template_contour)
                    
                    for scene_contour in scene_contours:
                        scene_area = cv2.contourArea(scene_contour)
                        
                        # Multi-criteria contour matching
                        if scene_area > 200:  # Reduced minimum area threshold
                            # 1. Hu moments shape matching
                            try:
                                shape_distance = cv2.matchShapes(template_contour, scene_contour, 1, 0.0)
                                
                                # 2. Area similarity
                                if template_area > 0:
                                    area_ratio = min(scene_area, template_area) / max(scene_area, template_area)
                                else:
                                    area_ratio = 0.0
                                
                                # 3. Perimeter similarity
                                template_perimeter = cv2.arcLength(template_contour, True)
                                scene_perimeter = cv2.arcLength(scene_contour, True)
                                if template_perimeter > 0:
                                    perimeter_ratio = min(scene_perimeter, template_perimeter) / max(scene_perimeter, template_perimeter)
                                else:
                                    perimeter_ratio = 0.0
                                
                                # Combined scoring
                                if shape_distance < best_shape_match:
                                    best_shape_match = shape_distance
                                    best_area_match = (area_ratio + perimeter_ratio) / 2.0
                                    
                            except:
                                continue
                    
                    # Enhanced confidence calculation
                    if best_shape_match < 1.0:  # More lenient threshold
                        shape_score = max(0.0, 1.0 - best_shape_match)
                        area_score = best_area_match
                        
                        # Combined confidence with area weighting
                        confidences['contour'] = min(1.0, (shape_score * 0.7 + area_score * 0.3) * 1.2)
                        
            except Exception as e:
                pass
            
            # 7. ENHANCED Edge-Based Template Matching (FIXED)
            confidences['edges'] = 0.0
            try:
                # Generate or use template edges
                template_edges = template_data.get('edge_template')
                if template_edges is None:
                    # Generate template edges on-the-fly
                    template_rgb = template_data.get('rgb')
                    if template_rgb is not None:
                        template_gray = cv2.cvtColor(template_rgb, cv2.COLOR_BGR2GRAY)
                        template_edges = cv2.Canny(template_gray, 50, 150)
                
                if template_edges is not None:
                    # Multi-method edge matching for robustness
                    edge_confidences = []
                    
                    # Method 1: Direct template matching
                    try:
                        edge_result = cv2.matchTemplate(scene_edges, template_edges, cv2.TM_CCOEFF_NORMED)
                        _, edge_max1, _, edge_loc = cv2.minMaxLoc(edge_result)
                        edge_confidences.append(edge_max1)
                        
                        # Store location
                        h_edge, w_edge = template_edges.shape
                        locations['edges'] = (edge_loc[0], edge_loc[1], w_edge, h_edge)
                    except:
                        edge_confidences.append(0.0)
                    
                    # Method 2: Edge density comparison
                    try:
                        template_edge_density = np.sum(template_edges > 0) / template_edges.size
                        scene_edge_density = np.sum(scene_edges > 0) / scene_edges.size
                        
                        if template_edge_density > 0:
                            density_similarity = 1.0 - abs(template_edge_density - scene_edge_density) / template_edge_density
                            edge_confidences.append(max(0.0, density_similarity))
                        else:
                            edge_confidences.append(0.0)
                    except:
                        edge_confidences.append(0.0)
                    
                    # Method 3: Edge orientation similarity
                    try:
                        # Compute edge orientations
                        sobel_x_template = cv2.Sobel(template_edges, cv2.CV_64F, 1, 0, ksize=3)
                        sobel_y_template = cv2.Sobel(template_edges, cv2.CV_64F, 0, 1, ksize=3)
                        template_orientations = np.arctan2(sobel_y_template, sobel_x_template)
                        
                        sobel_x_scene = cv2.Sobel(scene_edges, cv2.CV_64F, 1, 0, ksize=3)
                        sobel_y_scene = cv2.Sobel(scene_edges, cv2.CV_64F, 0, 1, ksize=3)
                        scene_orientations = np.arctan2(sobel_y_scene, sobel_x_scene)
                        
                        # Compare orientation histograms
                        template_hist = np.histogram(template_orientations[template_edges > 0], bins=8)[0]
                        scene_hist = np.histogram(scene_orientations[scene_edges > 0], bins=8)[0]
                        
                        # Normalize histograms
                        if np.sum(template_hist) > 0 and np.sum(scene_hist) > 0:
                            template_hist = template_hist.astype(np.float32) / np.sum(template_hist)
                            scene_hist = scene_hist.astype(np.float32) / np.sum(scene_hist)
                            
                            # Calculate correlation
                            orientation_similarity = cv2.compareHist(template_hist, scene_hist, cv2.HISTCMP_CORREL)
                            edge_confidences.append(max(0.0, orientation_similarity))
                        else:
                            edge_confidences.append(0.0)
                    except:
                        edge_confidences.append(0.0)
                    
                    # Combine all edge matching methods
                    if edge_confidences:
                        # Weighted combination favoring template matching
                        weights = [0.6, 0.2, 0.2]  # Template matching gets highest weight
                        final_edge_confidence = sum(w * c for w, c in zip(weights, edge_confidences[:3]))
                        confidences['edges'] = min(1.0, final_edge_confidence * 1.3)  # Boost overall
                        
            except Exception as e:
                pass
            
            # 8. ACTIVATED Complex Features System (FIXED)
            confidences['features'] = 0.0
            if template_name in self.template_features:
                try:
                    # Extract scene features for comparison
                    x, y, w, h = best_location
                    scene_rgb_roi = rgb[y:y+h, x:x+w] if (y+h < rgb.shape[0] and x+w < rgb.shape[1]) else None
                    scene_depth_roi = depth[y:y+h, x:x+w] if (y+h < depth.shape[0] and x+w < depth.shape[1]) else None
                    
                    if scene_rgb_roi is not None and scene_depth_roi is not None:
                        template_features = self.template_features[template_name]
                        
                        # Quick feature extraction for scene
                        scene_gray = cv2.cvtColor(scene_rgb_roi, cv2.COLOR_BGR2GRAY)
                        
                        # 1. Color histogram comparison
                        color_confidence = 0.0
                        if 'color_histogram' in template_features:
                            try:
                                scene_hsv = cv2.cvtColor(scene_rgb_roi, cv2.COLOR_BGR2HSV)
                                scene_hist_h = cv2.calcHist([scene_hsv], [0], None, [30], [0, 180])
                                scene_hist_s = cv2.calcHist([scene_hsv], [1], None, [32], [0, 256])
                                
                                # Normalize
                                scene_hist_h = scene_hist_h.astype(np.float32) / (np.sum(scene_hist_h) + 1e-7)
                                scene_hist_s = scene_hist_s.astype(np.float32) / (np.sum(scene_hist_s) + 1e-7)
                                
                                # Compare with template
                                template_color = template_features['color_histogram']
                                if 'hue' in template_color and 'saturation' in template_color:
                                    template_h = np.array(template_color['hue']).astype(np.float32)
                                    template_s = np.array(template_color['saturation']).astype(np.float32)
                                    
                                    h_corr = cv2.compareHist(scene_hist_h.flatten(), template_h, cv2.HISTCMP_CORREL)
                                    s_corr = cv2.compareHist(scene_hist_s.flatten(), template_s, cv2.HISTCMP_CORREL)
                                    
                                    color_confidence = max(0.0, (h_corr + s_corr) / 2.0)
                            except:
                                pass
                        
                        # 2. Texture comparison
                        texture_confidence = 0.0
                        if 'texture_lbp' in template_features:
                            try:
                                # Simple texture via variance
                                kernel = np.ones((5,5), np.float32) / 25
                                blurred = cv2.filter2D(scene_gray.astype(np.float32), -1, kernel)
                                texture_var = cv2.filter2D((scene_gray.astype(np.float32) - blurred)**2, -1, kernel)
                                texture_hist = np.histogram(texture_var, bins=20)[0]
                                texture_hist_norm = texture_hist.astype(np.float32) / (np.sum(texture_hist) + 1e-7)
                                
                                template_texture = np.array(template_features['texture_lbp']).astype(np.float32)
                                texture_confidence = max(0.0, cv2.compareHist(texture_hist_norm, template_texture, cv2.HISTCMP_CORREL))
                            except:
                                pass
                        
                        # 3. Depth statistics comparison
                        depth_confidence = 0.0
                        if 'depth_variance' in template_features:
                            try:
                                scene_valid_depth = scene_depth_roi[scene_depth_roi > 0]
                                if len(scene_valid_depth) > 20:
                                    scene_depth_stats = {
                                        'variance': float(np.var(scene_valid_depth)),
                                        'std': float(np.std(scene_valid_depth)),
                                        'mean': float(np.mean(scene_valid_depth))
                                    }
                                    
                                    template_depth_stats = template_features['depth_variance']
                                    
                                    # Compare statistics
                                    var_diff = abs(scene_depth_stats['variance'] - template_depth_stats['variance'])
                                    var_score = 1.0 / (1.0 + var_diff / 1000.0)  # Normalize
                                    
                                    std_diff = abs(scene_depth_stats['std'] - template_depth_stats['std'])
                                    std_score = 1.0 / (1.0 + std_diff / 100.0)  # Normalize
                                    
                                    depth_confidence = (var_score + std_score) / 2.0
                            except:
                                pass
                        
                        # Combine all complex feature confidences
                        feature_scores = [color_confidence, texture_confidence, depth_confidence]
                        valid_scores = [score for score in feature_scores if score > 0]
                        
                        if valid_scores:
                            confidences['features'] = min(1.0, np.mean(valid_scores) * 1.1)  # Slight boost
                        
                except Exception as e:
                    pass
            
            # === ULTIMATE 9-MODALITY ADAPTIVE WEIGHT ASSIGNMENT (UPGRADED) ===
            sift_count = template_data.get('sift_count', 0)
            akaze_count = template_data.get('akaze_count', 0)
            kaze_count = template_data.get('kaze_count', 0)
            
            if sift_count > 30 or akaze_count > 40 or kaze_count > 35:
                # High feature template - ALL 9 ALGORITHMS OPTIMIZED
                weights = {
                    'rgb_template': 0.16,  # Excellent performer
                    'sift': 0.30,          # Top performer (1.00)
                    'akaze': 0.15,         # UPGRADED from ORB - better performance
                    'kaze': 0.12,          # NEW - Research shows best traditional
                    'depth': 0.12,         # Solid performer
                    'ir': 0.07,            # Good performer
                    'contour': 0.04,       # FIXED - should work now
                    'edges': 0.03,         # FIXED - enhanced
                    'features': 0.01       # ACTIVATED - complex features
                }
            elif sift_count > 10 or akaze_count > 15 or kaze_count > 12:
                # Moderate feature template - ALL 9 ALGORITHMS BALANCED
                weights = {
                    'rgb_template': 0.20,  # Strong performer
                    'sift': 0.25,          # Excellent performer
                    'akaze': 0.12,         # UPGRADED algorithm
                    'kaze': 0.10,          # BEST traditional algorithm
                    'depth': 0.13,         # Reliable method
                    'ir': 0.08,            # Decent performer
                    'contour': 0.06,       # FIXED - should contribute
                    'edges': 0.04,         # ENHANCED matching
                    'features': 0.02       # Complex feature analysis
                }
            else:
                # Low feature template - ALL 9 ALGORITHMS ADAPTED
                weights = {
                    'rgb_template': 0.25,  # Primary method for low features
                    'sift': 0.18,          # Still valuable
                    'akaze': 0.10,         # UPGRADED - better than ORB
                    'kaze': 0.08,          # BEST for difficult objects
                    'depth': 0.16,         # Important for featureless objects
                    'ir': 0.09,            # Good for material detection
                    'contour': 0.07,       # FIXED - shape is important
                    'edges': 0.05,         # ENHANCED edge detection
                    'features': 0.02       # Statistical features
                }
            
            # === ADVANCED CONFIDENCE FUSION with OpenCV Techniques ===
            
            # 1. Confidence Calibration using Platt Scaling
            calibrated_confidences = {}
            for modality, conf in confidences.items():
                # Less aggressive sigmoid calibration for better sensitivity
                calibrated_conf = 1.0 / (1.0 + np.exp(-3.0 * (conf - 0.4)))
                calibrated_confidences[modality] = calibrated_conf
            
            # 2. Cross-modality validation
            modality_agreement = 0.0
            active_modalities = [k for k, v in calibrated_confidences.items() if v > 0.3]
            if len(active_modalities) > 1:
                # Calculate agreement between modalities
                conf_values = [calibrated_confidences[k] for k in active_modalities]
                # Fixed divide by zero warning
                conf_mean = np.mean(conf_values)
                conf_std = np.std(conf_values)
                if conf_mean > 1e-8:
                    modality_agreement = 1.0 - (conf_std / conf_mean)
                else:
                    modality_agreement = 0.0
                modality_agreement = max(0.0, min(1.0, modality_agreement))
            
            # 3. Weighted fusion with FAR OBJECT agreement boost
            base_confidence = sum(weights[mod] * calibrated_confidences[mod] for mod in weights.keys())
            agreement_boost = modality_agreement * 0.1  # Up to 10% boost for agreement
            
            # FAR OBJECT BOOST: If detection seems far (low confidence), boost reliable methods
            far_object_boost = 0.0
            if base_confidence < 0.65:
                # Boost SIFT and RGB for far objects (most reliable at distance)
                if calibrated_confidences.get('sift', 0) > 0.8:
                    far_object_boost += 0.15  # SIFT boost for far objects
                if calibrated_confidences.get('rgb_template', 0) > 0.6:
                    far_object_boost += 0.10  # RGB boost for far objects
            
            # 4. Adaptive confidence based on template characteristics
            sift_count = template_data.get('sift_count', 0)
            if sift_count > 20:
                # High texture - boost SIFT and reduce template matching uncertainty
                texture_factor = 1.05
            elif sift_count < 5:
                # Low texture - boost depth/IR and add stability
                texture_factor = 1.02
            else:
                texture_factor = 1.0
            
            # 5. WEIGHTED FUSION - Calculate final confidence using weights
            weighted_confidence = 0.0
            total_weight = 0.0
            
            for modality, confidence in calibrated_confidences.items():
                if modality in self.matcher.weights:
                    weight = self.matcher.weights[modality]
                    weighted_confidence += confidence * weight
                    total_weight += weight
            
            # Normalize and apply boosts
            base_weighted = weighted_confidence / total_weight if total_weight > 0 else 0.0
            final_confidence = (base_weighted + agreement_boost + far_object_boost) * texture_factor
            final_confidence = min(1.0, max(0.0, final_confidence))  # Clamp to [0,1]
            
            # 6. Smart location selection with confidence weighting
            location_candidates = [(k, v, locations.get(k, locations['rgb_template'])) 
                                 for k, v in calibrated_confidences.items() if v > 0.2]
            
            if location_candidates:
                # Ultimate 9-modality reliability weights
                modality_reliability = {
                    'sift': 1.0,      # Highest reliability for geometric accuracy
                    'akaze': 0.95,    # Very reliable, upgraded from ORB
                    'depth': 0.9,     # High reliability for 3D structure
                    'contour': 0.85,  # Good for shape-based objects
                    'edges': 0.8,     # Reliable for structured objects
                    'rgb_template': 0.75, # Standard template matching
                    'ir': 0.7,       # Good for material detection
                    'features': 0.6  # Complex features, moderate reliability
                }
                weighted_scores = [(k, v * modality_reliability.get(k, 0.5), loc) for k, v, loc in location_candidates]
                best_modality, _, best_location = max(weighted_scores, key=lambda x: x[1])
            else:
                best_modality = max(confidences.keys(), key=lambda k: confidences[k])
                best_location = locations.get(best_modality, locations['rgb_template'])
            
            # === INDUSTRIAL VALIDATION WITH UPGRADED ALGORITHMS ===
            # 1. Adaptive confidence thresholds based on template quality
            sift_count = template_data.get('sift_count', 0)
            akaze_count = template_data.get('akaze_count', 0)
            kaze_count = template_data.get('kaze_count', 0)
            
            # Enhanced threshold logic with new algorithms
            total_features = sift_count + akaze_count + kaze_count
            if total_features > 50:  # High feature template
                very_strong_modalities = [k for k, v in calibrated_confidences.items() if v > 0.7]
                strong_modalities = [k for k, v in calibrated_confidences.items() if v > 0.5]
            else:  # Low feature template - rely more on other methods
                very_strong_modalities = [k for k, v in calibrated_confidences.items() if v > 0.6]
                strong_modalities = [k for k, v in calibrated_confidences.items() if v > 0.4]
            
            # 2. Enhanced feature validation with upgraded algorithms
            feature_validation = True
            if confidences.get('sift', 0) > 0.8 and sift_count > 5:
                # SIFT validation only for high confidence + sufficient features
                feature_validation = confidences.get('sift', 0) > 0.85
            
            if confidences.get('akaze', 0) > 0.7 and akaze_count > 8:
                # AKAZE validation (better than ORB)
                feature_validation = feature_validation and confidences.get('akaze', 0) > 0.75
                
            if confidences.get('kaze', 0) > 0.7 and kaze_count > 6:
                # KAZE validation (best traditional algorithm)
                feature_validation = feature_validation and confidences.get('kaze', 0) > 0.8
            
            # 3. Adaptive depth validation
            depth_validation = True
            if confidences.get('depth', 0) > 0.6:  # Only strict validate high confidence
                template_depth = template_data.get('depth')
                if template_depth is not None:
                    x, y, w, h = best_location
                    scene_depth_roi = depth[y:y+h, x:x+w] if (y+h < depth.shape[0] and x+w < depth.shape[1]) else None
                    if scene_depth_roi is not None:
                        template_valid_depth = template_depth[template_depth > 0]
                        scene_valid_depth = scene_depth_roi[scene_depth_roi > 0]
                        
                        # Require reasonable depth data
                        if len(template_valid_depth) > 50 and len(scene_valid_depth) > 50:
                            template_depth_mean = np.mean(template_valid_depth)
                            scene_depth_mean = np.mean(scene_valid_depth)
                            
                            # Allow reasonable depth variation
                            if template_depth_mean > 0:
                                depth_diff = abs(template_depth_mean - scene_depth_mean) / template_depth_mean
                                depth_validation = depth_diff < 0.4  # 40% tolerance
            
            # 4. Reasonable template matching validation
            template_validation = True
            if confidences.get('rgb_template', 0) > 0.7:
                # Template match should be strong
                template_validation = confidences.get('rgb_template', 0) > 0.75
            
            # 5. Background subtraction validation - check if object actually present
            background_validation = self._has_significant_object(rgb, depth, best_location)
            
            # 5. SIZE validation - object must be reasonable size
            x, y, w, h = best_location
            size_validation = (w > 30 and h > 30 and w < depth.shape[1]*0.8 and h < depth.shape[0]*0.8)
            
            # SINGLE THRESHOLD VALIDATION - Simple and reliable
            valid_detection = (final_confidence >= self.matcher.MASTER_THRESHOLD and 
                             size_validation)  # Only check confidence and reasonable size
            
            if valid_detection:
                # Extract ROI point cloud for 6D pose estimation
                x, y, w, h = best_location
                pose_6d = None
                
                if depth is not None:
                    try:
                        # Extract point cloud from detected ROI for 6D pose
                        roi_pointcloud = self.depth_to_pointcloud(depth, (x, y, w, h))
                        
                        # Estimate 6D pose using ICP if we have sufficient points
                        if roi_pointcloud is not None and len(roi_pointcloud) >= 50:
                            print(f"ü§ñ Attempting 6D pose estimation with {len(roi_pointcloud)} points...")
                            pose_6d = self.pose_estimator.estimate_6d_pose(template_name, roi_pointcloud)
                            if pose_6d:
                                print(f"ü§ñ 6D POSE {template_name}: X:{pose_6d['position']['x']:.3f} Y:{pose_6d['position']['y']:.3f} Z:{pose_6d['position']['z']:.3f} "
                                      f"Roll:{pose_6d['orientation']['roll']:.1f}¬∞ Pitch:{pose_6d['orientation']['pitch']:.1f}¬∞ Yaw:{pose_6d['orientation']['yaw']:.1f}¬∞")
                            else:
                                print(f"‚ö†Ô∏è  6D pose estimation returned None for {template_name}")
                        elif roi_pointcloud is not None:
                            print(f"‚ö†Ô∏è  Insufficient points for 6D pose: {len(roi_pointcloud)} < 50")
                        else:
                            print(f"‚ö†Ô∏è  No point cloud extracted from ROI for {template_name}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  6D pose estimation failed: {e}")
                
                detections.append({
                    'template_name': template_name,
                    'confidence': float(final_confidence),
                    'location': best_location,
                    'pose_6d': pose_6d,  # NEW: 6D pose information
                    'feature_details': {
                        'final_confidence': final_confidence,
                        'rgb_template': confidences['rgb_template'],
                        'sift': confidences['sift'],
                        'akaze': confidences['akaze'],
                        'kaze': confidences['kaze'],
                        'depth': confidences['depth'],
                        'ir': confidences['ir'],
                        'contour': confidences['contour'],
                        'edges': confidences['edges'],
                        'features': confidences['features'],
                        'best_modality': best_modality,
                        'weights_used': weights,
                        'sift_features': sift_count,
                        'akaze_features': akaze_count,
                        'kaze_features': kaze_count,
                        'modality_agreement': modality_agreement
                    }
                })
                
                print(f"üéØ 4-MODAL FAR-DISTANCE Detection {template_name}: {final_confidence:.3f} "
                      f"(RGB:{confidences['rgb_template']:.2f} SIFT:{confidences['sift']:.2f} "
                      f"IR:{confidences['ir']:.2f} Contour:{confidences['contour']:.2f} ‚Üí {best_modality})")
        
        return detections
    
    def draw_detections(self, frame: np.ndarray, detections: List[Dict]):
        """Draw unified fusion detection results with 6D pose visualization"""
        for detection in detections:
            x, y, w, h = detection['location']
            confidence = detection['confidence']
            name = detection['template_name']
            details = detection.get('feature_details', {})
            pose_6d = detection.get('pose_6d')
            
            # Color coding for all 9 modalities
            best_modality = details.get('best_modality', 'rgb_template')
            modality_colors = {
                'sift': (0, 255, 0),        # Green - Feature matching
                'akaze': (0, 200, 0),       # Dark Green - AKAZE features (upgraded from ORB)
                'kaze': (0, 180, 0),        # Dark Green - KAZE features (new algorithm)
                'depth': (255, 0, 0),       # Blue - 3D structure
                'contour': (255, 165, 0),   # Orange - Shape matching
                'edges': (255, 255, 0),     # Yellow - Edge matching
                'rgb_template': (0, 255, 255), # Cyan - Template matching
                'ir': (255, 0, 255),        # Magenta - IR/thermal
                'features': (128, 0, 128)   # Purple - Complex features
            }
            color = modality_colors.get(best_modality, (255, 255, 255))
            
            # Draw bounding box
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
            
            # Draw 6D pose coordinate axes if available
            if pose_6d is not None:
                self._draw_6d_pose_axes(frame, pose_6d, (x, y, w, h))
            
            # Draw main label with 9-modality indicator
            label = f"{name} [9-MODAL + 6D POSE]" if pose_6d else f"{name} [9-MODAL]"
            cv2.putText(frame, label, (x, y - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # Draw unified confidence with best modality
            conf_text = f"FUSED: {confidence:.3f} ({best_modality.upper()})"
            cv2.putText(frame, conf_text, (x, y + h + 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            # Draw 6D pose information
            if pose_6d is not None:
                pose_text = (f"X:{pose_6d['position']['x']:.3f} Y:{pose_6d['position']['y']:.3f} Z:{pose_6d['position']['z']:.3f}")
                cv2.putText(frame, pose_text, (x, y + h + 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
                
                orientation_text = (f"R:{pose_6d['orientation']['roll']:.1f}¬∞ P:{pose_6d['orientation']['pitch']:.1f}¬∞ Y:{pose_6d['orientation']['yaw']:.1f}¬∞")
                cv2.putText(frame, orientation_text, (x, y + h + 55), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
            
            # Draw comprehensive modality breakdown
            if details:
                # Line 1: Feature-based methods
                breakdown1 = (f"SIFT:{details.get('sift', 0):.2f} "
                            f"AKAZE:{details.get('akaze', 0):.2f} "
                            f"KAZE:{details.get('kaze', 0):.2f} "
                            f"RGB:{details.get('rgb_template', 0):.2f}")
                y_offset = 70 if pose_6d else 40
                cv2.putText(frame, breakdown1, (x, y + h + y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                
                # Line 2: Shape and depth methods
                breakdown2 = (f"D:{details.get('depth', 0):.2f} "
                            f"C:{details.get('contour', 0):.2f} "
                            f"E:{details.get('edges', 0):.2f} "
                            f"IR:{details.get('ir', 0):.2f}")
                y_offset = 85 if pose_6d else 55
                cv2.putText(frame, breakdown2, (x, y + h + y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    def _draw_6d_pose_axes(self, frame: np.ndarray, pose_6d: dict, bbox: tuple):
        """Draw 3D coordinate axes showing RELATIVE 6D pose orientation from template"""
        x, y, w, h = bbox
        center_x = x + w // 2
        center_y = y + h // 2
        
        # Extract RELATIVE pose information (relative to template)
        pos = pose_6d['position']
        orient = pose_6d['orientation']
        
        # Convert RELATIVE angles to radians
        import math
        roll = math.radians(orient['roll'])
        pitch = math.radians(orient['pitch']) 
        yaw = math.radians(orient['yaw'])
        
        # Create RELATIVE rotation matrix from Euler angles (ZYX order)
        cos_r, sin_r = math.cos(roll), math.sin(roll)
        cos_p, sin_p = math.cos(pitch), math.sin(pitch)
        cos_y, sin_y = math.cos(yaw), math.sin(yaw)
        
        # RELATIVE rotation matrix (shows change from template orientation)
        R = np.array([
            [cos_y*cos_p, cos_y*sin_p*sin_r - sin_y*cos_r, cos_y*sin_p*cos_r + sin_y*sin_r],
            [sin_y*cos_p, sin_y*sin_p*sin_r + cos_y*cos_r, sin_y*sin_p*cos_r - cos_y*sin_r],
            [-sin_p, cos_p*sin_r, cos_p*cos_r]
        ])
        
        # 3D axis vectors scaled for visibility
        axis_length = min(w, h) // 3
        
        # Standard coordinate axes (identity orientation = template orientation)
        axes_3d = np.array([
            [axis_length, 0, 0],  # X-axis (Red)
            [0, axis_length, 0],  # Y-axis (Green) 
            [0, 0, axis_length]   # Z-axis (Blue)
        ])
        
        # Apply RELATIVE rotation to show change from template
        rotated_axes = (R @ axes_3d.T).T
        
        # FIXED: Project 3D axes to 2D properly
        axis_colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0)]  # BGR: Red=X, Green=Y, Blue=Z
        axis_labels = ['X', 'Y', 'Z']
        
        # Draw coordinate system origin (detection center)
        cv2.circle(frame, (center_x, center_y), 8, (255, 255, 255), -1)
        cv2.circle(frame, (center_x, center_y), 10, (0, 0, 0), 2)
        
        for i, (axis, color, label) in enumerate(zip(rotated_axes, axis_colors, axis_labels)):
            # Project 3D to 2D (orthographic projection)
            # Scale down Z component for depth visualization
            end_x = int(center_x + axis[0] + axis[2] * 0.3)  # Add Z perspective
            end_y = int(center_y - axis[1])  # Flip Y for screen coordinates
            
            # Draw axis arrow with thickness based on Z depth
            thickness = max(2, int(4 + axis[2] * 0.01))  # Thicker = closer
            cv2.arrowedLine(frame, (center_x, center_y), (end_x, end_y), color, thickness, tipLength=0.3)
            
            # Draw axis label
            label_x = int(center_x + axis[0] * 1.3 + axis[2] * 0.4)
            label_y = int(center_y - axis[1] * 1.3)
            cv2.putText(frame, label, (label_x, label_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        # Draw pose difference values near axes
        pose_text = f"ŒîX:{pos['x']:.3f} ŒîY:{pos['y']:.3f} ŒîZ:{pos['z']:.3f}"
        cv2.putText(frame, pose_text, (center_x - w//2, center_y - h//2 - 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        rotation_text = f"ŒîR:{orient['roll']:.1f}¬∞ ŒîP:{orient['pitch']:.1f}¬∞ ŒîY:{orient['yaw']:.1f}¬∞"
        cv2.putText(frame, rotation_text, (center_x - w//2, center_y - h//2 - 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
    
    def mouse_callback(self, event, x, y, flags, param):
        """Mouse callback for template creation"""
        if not self.creating_template:
            return
            
        if event == cv2.EVENT_LBUTTONDOWN:
            self.start_point = (x, y)
        elif event == cv2.EVENT_LBUTTONUP:
            self.end_point = (x, y)
    
    def _rift_enhance_ir(self, ir_image: np.ndarray) -> np.ndarray:
        """RIFT: Multi-scale guided filtering for IR enhancement"""
        try:
            # Convert to float for processing
            ir_float = ir_image.astype(np.float32) / 255.0
            
            # Multi-scale Gaussian filtering for radiation invariance
            scales = [1, 2, 4]
            enhanced_layers = []
            
            for scale in scales:
                # Gaussian blur at different scales
                kernel_size = 2 * scale + 1
                blurred = cv2.GaussianBlur(ir_float, (kernel_size, kernel_size), scale)
                
                # Edge-preserving enhancement
                enhanced = ir_float / (blurred + 0.01)  # Avoid divide by zero
                enhanced_layers.append(enhanced)
            
            # Combine scales with adaptive weighting
            final_enhanced = np.zeros_like(ir_float)
            weights = [0.5, 0.3, 0.2]  # Emphasize finer details
            
            for i, layer in enumerate(enhanced_layers):
                final_enhanced += weights[i] * layer
            
            # Normalize and convert back
            final_enhanced = np.clip(final_enhanced, 0, 1)
            return (final_enhanced * 255).astype(np.uint8)
            
        except Exception as e:
            return ir_image  # Fallback to original
    
    def _rift_template_match(self, template: np.ndarray, scene: np.ndarray) -> float:
        """RIFT: Radiation Insensitive Feature Transform matching"""
        try:
            # RIFT: Normalized cross-correlation with radiation invariance
            template_norm = cv2.normalize(template.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
            scene_norm = cv2.normalize(scene.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
            
            # Multi-method matching for robustness
            methods = [cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR_NORMED]
            confidences = []
            
            for method in methods:
                try:
                    result = cv2.matchTemplate(scene_norm, template_norm, method)
                    _, max_val, _, _ = cv2.minMaxLoc(result)
                    confidences.append(float(max_val))
                except:
                    confidences.append(0.0)
            
            # Weighted combination (CCOEFF_NORMED is more reliable)
            if len(confidences) >= 2:
                return 0.7 * confidences[0] + 0.3 * confidences[1]
            elif len(confidences) == 1:
                return confidences[0]
            else:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _rift_enhance_ir_fast(self, ir_image: np.ndarray) -> np.ndarray:
        """FAST RIFT: Single-scale guided filtering for real-time"""
        try:
            # Convert to float for processing
            ir_float = ir_image.astype(np.float32) / 255.0
            
            # Single-scale Gaussian filtering (much faster)
            blurred = cv2.GaussianBlur(ir_float, (5, 5), 2)
            
            # Edge-preserving enhancement
            enhanced = ir_float / (blurred + 0.01)
            
            # Normalize and convert back
            enhanced = np.clip(enhanced, 0, 1)
            return (enhanced * 255).astype(np.uint8)
            
        except Exception as e:
            return ir_image
    
    def _rift_template_match_fast(self, template: np.ndarray, scene: np.ndarray) -> float:
        """FAST RIFT: Single method template matching"""
        try:
            # Normalized template matching (fastest method)
            template_norm = cv2.normalize(template.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
            scene_norm = cv2.normalize(scene.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)
            
            result = cv2.matchTemplate(scene_norm, template_norm, cv2.TM_CCOEFF_NORMED)
            _, max_val, _, _ = cv2.minMaxLoc(result)
            return float(max_val)
                
        except Exception as e:
            return 0.0
    
    def _arrch_template_match_fast(self, template: np.ndarray, scene: np.ndarray) -> Tuple[float, Tuple[int, int]]:
        """SIMPLE: Fast template matching for far objects"""
        try:
            th, tw = template.shape
            sh, sw = scene.shape
            
            if th >= sh or tw >= sw or th < 10 or tw < 10:
                return 0.0, (0, 0)
            
            # Simple multi-scale for far objects (much faster)
            best_score = 0.0
            best_location = (0, 0)
            
            # AGGRESSIVE scales for 2+ meter detection
            scales = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0, 1.2, 1.5]
            
            for scale in scales:
                try:
                    new_h, new_w = int(th * scale), int(tw * scale)
                    if new_h < 8 or new_w < 8 or new_h >= sh or new_w >= sw:
                        continue
                    
                    scaled_template = cv2.resize(template, (new_w, new_h))
                    
                    # Simple OpenCV template matching (much faster)
                    result = cv2.matchTemplate(scene, scaled_template, cv2.TM_CCOEFF_NORMED)
                    _, max_val, _, max_loc = cv2.minMaxLoc(result)
                    
                    if max_val > best_score:
                        best_score = max_val
                        best_location = max_loc
                        
                except:
                    continue
            
            return float(best_score), best_location
            
        except Exception as e:
            return 0.0, (0, 0)
    
    def _compute_amns_similarity(self, template: np.ndarray, scene: np.ndarray) -> Tuple[float, Tuple[int, int]]:
        """A-MNS: Adaptive Majority Neighbour Similarity (4.4x faster, more robust)"""
        try:
            th, tw = template.shape
            sh, sw = scene.shape
            
            if th >= sh or tw >= sw:
                return 0.0, (0, 0)
            
            best_score = 0.0
            best_location = (0, 0)
            
            # A-MNS sampling strategy (much faster than full template matching)
            step_y = max(1, (sh - th) // 20)  # Sample every 20th position
            step_x = max(1, (sw - tw) // 20)
            
            for y in range(0, sh - th + 1, step_y):
                for x in range(0, sw - tw + 1, step_x):
                    # Extract candidate region
                    candidate = scene[y:y+th, x:x+tw]
                    
                    if candidate.shape != template.shape:
                        continue
                    
                    # A-MNS: Majority Neighbour Similarity computation
                    amns_score = self._amns_compute_similarity(template, candidate)
                    
                    if amns_score > best_score:
                        best_score = amns_score
                        best_location = (x, y)
            
            return best_score, best_location
            
        except Exception as e:
            return 0.0, (0, 0)
    
    def _amns_compute_similarity(self, template: np.ndarray, candidate: np.ndarray) -> float:
        """A-MNS core similarity computation (rotation/scale invariant)"""
        try:
            # A-MNS: Majority neighbour pattern matching
            h, w = template.shape
            
            # Sample key points for efficiency (A-MNS approach)
            sample_points = min(100, h * w // 4)  # Sample 25% of pixels
            
            indices = np.random.choice(h * w, sample_points, replace=False)
            rows, cols = np.unravel_index(indices, (h, w))
            
            matches = 0
            total = 0
            
            for i, j in zip(rows, cols):
                # Skip border pixels (need neighbours)
                if i == 0 or i == h-1 or j == 0 or j == w-1:
                    continue
                
                # A-MNS: 8-neighbour pattern matching
                template_neighbors = [
                    template[i-1, j-1], template[i-1, j], template[i-1, j+1],
                    template[i, j-1],                    template[i, j+1],
                    template[i+1, j-1], template[i+1, j], template[i+1, j+1]
                ]
                
                candidate_neighbors = [
                    candidate[i-1, j-1], candidate[i-1, j], candidate[i-1, j+1],
                    candidate[i, j-1],                      candidate[i, j+1],
                    candidate[i+1, j-1], candidate[i+1, j], candidate[i+1, j+1]
                ]
                
                # A-MNS: Majority pattern similarity
                template_pattern = np.array(template_neighbors) > template[i, j]
                candidate_pattern = np.array(candidate_neighbors) > candidate[i, j]
                
                # Count matching pattern bits
                pattern_matches = np.sum(template_pattern == candidate_pattern)
                matches += pattern_matches
                total += 8  # 8 neighbors
            
            # A-MNS similarity score
            if total > 0:
                similarity = matches / total
                return similarity
            else:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _arrch_template_match(self, template: np.ndarray, scene: np.ndarray) -> Tuple[float, Tuple[int, int]]:
        """ARRCH: Adaptive Radial Ring Code Histograms for scale-invariant RGB matching"""
        try:
            # Preprocessing for ARRCH
            template_norm = cv2.equalizeHist(template.astype(np.uint8))
            scene_norm = cv2.equalizeHist(scene.astype(np.uint8))
            
            # ARRCH descriptor computation
            template_arrch = self._compute_arrch_descriptor(template_norm)
            
            # Sliding window approach with ARRCH matching
            th, tw = template.shape
            sh, sw = scene.shape
            
            if th >= sh or tw >= sw:
                return 0.0, (0, 0)
            
            best_confidence = 0.0
            best_location = (0, 0)
            
            # Adaptive step size based on template size
            step_y = max(1, th // 8)
            step_x = max(1, tw // 8)
            
            for y in range(0, sh - th + 1, step_y):
                for x in range(0, sw - tw + 1, step_x):
                    # Extract scene window
                    scene_window = scene_norm[y:y+th, x:x+tw]
                    
                    if scene_window.shape == template_norm.shape:
                        # Compute ARRCH descriptor for scene window
                        scene_arrch = self._compute_arrch_descriptor(scene_window)
                        
                        # ARRCH similarity computation
                        confidence = self._compare_arrch_descriptors(template_arrch, scene_arrch)
                        
                        if confidence > best_confidence:
                            best_confidence = confidence
                            best_location = (x, y)
            
            return best_confidence, best_location
            
        except Exception as e:
            return 0.0, (0, 0)
    
    def _compute_arrch_descriptor(self, image: np.ndarray) -> np.ndarray:
        """Compute ARRCH (Adaptive Radial Ring Code Histograms) descriptor"""
        try:
            h, w = image.shape
            center_y, center_x = h // 2, w // 2
            
            # Adaptive ring parameters based on image size
            max_radius = min(center_y, center_x)
            num_rings = min(8, max_radius // 3)  # Adaptive number of rings
            num_sectors = 16  # Fixed number of angular sectors
            
            if num_rings < 2:
                return np.zeros(32)  # Fallback for very small images
            
            # Initialize ARRCH descriptor
            descriptor = []
            
            for ring in range(1, num_rings + 1):
                inner_radius = (ring - 1) * max_radius // num_rings
                outer_radius = ring * max_radius // num_rings
                
                # Extract ring intensities
                ring_intensities = []
                
                for angle in range(num_sectors):
                    angle_rad = 2 * np.pi * angle / num_sectors
                    
                    # Sample points in this sector of the ring
                    sector_values = []
                    for r in range(inner_radius, outer_radius + 1):
                        if r == 0:
                            continue
                        x = int(center_x + r * np.cos(angle_rad))
                        y = int(center_y + r * np.sin(angle_rad))
                        
                        if 0 <= x < w and 0 <= y < h:
                            sector_values.append(image[y, x])
                    
                    if sector_values:
                        ring_intensities.append(np.mean(sector_values))
                    else:
                        ring_intensities.append(0)
                
                # Compute histogram for this ring
                if ring_intensities:
                    ring_hist, _ = np.histogram(ring_intensities, bins=4, range=(0, 255))
                    ring_hist = ring_hist / (np.sum(ring_hist) + 1e-10)  # Normalize
                    descriptor.extend(ring_hist)
            
            return np.array(descriptor)
            
        except Exception as e:
            return np.zeros(32)  # Fallback descriptor
    
    def _compare_arrch_descriptors(self, desc1: np.ndarray, desc2: np.ndarray) -> float:
        """Compare two ARRCH descriptors using multiple similarity metrics"""
        try:
            if len(desc1) == 0 or len(desc2) == 0 or len(desc1) != len(desc2):
                return 0.0
            
            # Multi-metric ARRCH comparison
            confidences = []
            
            # 1. Cosine similarity (good for histogram data)
            if np.linalg.norm(desc1) > 0 and np.linalg.norm(desc2) > 0:
                cosine_sim = np.dot(desc1, desc2) / (np.linalg.norm(desc1) * np.linalg.norm(desc2))
                confidences.append(max(0, cosine_sim))
            
            # 2. Intersection similarity (robust for histograms)
            intersection = np.sum(np.minimum(desc1, desc2))
            union = np.sum(np.maximum(desc1, desc2))
            if union > 0:
                intersection_sim = intersection / union
                confidences.append(intersection_sim)
            
            # 3. Correlation coefficient
            if np.std(desc1) > 0 and np.std(desc2) > 0:
                correlation = np.corrcoef(desc1, desc2)[0, 1]
                if not np.isnan(correlation):
                    confidences.append(max(0, correlation))
            
            # Weighted combination
            if confidences:
                weights = [0.4, 0.35, 0.25][:len(confidences)]
                weights = np.array(weights) / np.sum(weights)
                return float(np.sum(np.array(confidences) * weights))
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def run(self):
        """Main detection loop"""
        if not self.initialize_camera():
            return
            
        print("\n" + "="*80)
        print("üß† COMPREHENSIVE MULTI-MODAL FEATURE EXTRACTION & WEIGHTED FUSION")
        print("="*80)
        print("Features:")
        print("  üé® RGB: Color histograms, edges, texture, geometry")
        print("  üìè Depth: Histograms, curvature, volume, variance")
        print("  üåê Point Cloud: 3D shape, normals, geometric primitives")
        print("  üî• IR: Material signatures, thermal properties")
        print("  ‚öñÔ∏è  Weighted Fusion: Intelligent confidence combination")
        print("\nControls:")
        print("  't' - Create comprehensive template")
        print("  's' - Save template")
        print("  'ESC' - Exit")
        print("="*80)
        
        cv2.namedWindow("Robust Multi-Modal Detection", cv2.WINDOW_NORMAL)
        cv2.setMouseCallback("Robust Multi-Modal Detection", self.mouse_callback)
        
        try:
            print("üîÑ Starting camera loop...")
            frame_counter = 0
            while True:
                try:
                    rgb, depth, ir = self.get_frames()
                    if rgb is None:
                        print(f"‚ö†Ô∏è  Frame {frame_counter}: No RGB data")
                        continue
                    
                    frame_counter += 1
                    if frame_counter % 30 == 0:  # Debug every 30 frames
                        print(f"‚úÖ Frame {frame_counter}: Got RGB {rgb.shape}, Depth: {depth.shape if depth is not None else None}")
                        
                except Exception as e:
                    print(f"‚ùå Frame capture error: {e}")
                    continue
                
                display_frame = rgb.copy()
                
                # Update FPS counter
                self.fps_counter += 1
                current_time = time.time()
                if current_time - self.fps_start_time >= 1.0:
                    self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
                    self.fps_counter = 0
                    self.fps_start_time = current_time
                
                if not self.creating_template:
                    # Detection mode
                    try:
                        detections = self.detect_objects(rgb, depth, ir)
                        self.draw_detections(display_frame, detections)
                    except Exception as e:
                        print(f"‚ùå Detection error: {e}")
                        detections = []
                    
                    # Status with FPS
                    status_text = f"Templates: {len(self.templates)} | Detections: {len(detections)} | FPS: {self.current_fps:.1f}"
                    cv2.putText(display_frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                    
                    # Show detection mode
                    cv2.putText(display_frame, "DETECTION MODE - Press 't' for new template", 
                               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # Performance indicator color coding
                    fps_color = (0, 255, 0) if self.current_fps >= 15 else (0, 255, 255) if self.current_fps >= 10 else (0, 0, 255)
                    cv2.putText(display_frame, f"PERF: {'GOOD' if self.current_fps >= 15 else 'OK' if self.current_fps >= 10 else 'SLOW'}", 
                               (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, fps_color, 2)
                    
                else:
                    # Template creation mode
                    cv2.putText(display_frame, f"Creating: {self.template_name}", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                    cv2.putText(display_frame, "Select object region, press 's' to save", 
                               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    if self.start_point and self.end_point:
                        cv2.rectangle(display_frame, self.start_point, self.end_point, (0, 255, 0), 2)
                
                cv2.imshow("Robust Multi-Modal Detection", display_frame)
                
                # Handle keyboard input
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC
                    break
                elif key == ord('t'):
                    self.template_name = f"robust_{int(time.time())}"
                    self.creating_template = True
                    self.start_point = None
                    self.end_point = None
                    print(f"üèóÔ∏è  Creating comprehensive template: {self.template_name}")
                elif key == ord('s') and self.creating_template and self.start_point and self.end_point:
                    # Create template
                    x1, y1 = self.start_point
                    x2, y2 = self.end_point
                    x1, x2 = min(x1, x2), max(x1, x2)
                    y1, y2 = min(y1, y2), max(y1, y2)
                    
                    if x2 - x1 > 30 and y2 - y1 > 30:
                        roi = (x1, y1, x2 - x1, y2 - y1)
                        self.create_template(self.template_name, rgb, depth, ir, roi)
                        
                        self.creating_template = False
                        self.start_point = None
                        self.end_point = None
                    else:
                        print("‚ö†Ô∏è  Selection too small!")
                
        finally:
            if self.pipeline:
                self.pipeline.stop()
            cv2.destroyAllWindows()


if __name__ == "__main__":
    detector = RobustMultiModalDetector()
    detector.run()